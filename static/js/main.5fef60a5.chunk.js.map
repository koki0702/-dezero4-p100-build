{"version":3,"sources":["images/check_gray.png","images/check_green.png","images/cover.jpg","images/check_wrong.png","images/check_green2.png","images/back_on.png","images/problems sync /^/.//.*$","images/problems/c10_1.png","images/problems/c10_9.png","images/problems/c1_1.png","images/problems/c2_1.png","images/problems/c2_2.png","images/problems/c2_3.png","images/problems/c3_1.png","images/problems/c3_2.png","images/problems/c3_3.png","images/problems/c3_4.png","images/problems/c3_5.png","images/problems/c3_6.png","images/problems/c3_7.png","images/problems/c4_1.png","images/problems/c4_2.png","images/problems/c4_3.png","images/problems/c4_4.png","images/problems/c4_5.png","images/problems/c5_1.png","images/problems/c5_2.png","images/problems/c5_3.png","images/problems/c6_1.png","images/problems/c6_2.png","images/problems/c6_3.png","images/problems/c6_4.png","images/problems/c6_5.png","images/problems/c7_1.png","images/problems/c7_2.png","images/problems/c7_3.png","images/problems/c7_4.png","images/problems/c7_5.png","images/problems/c8_1.png","images/problems/c8_2.png","images/problems/c8_3.png","images/problems/c9_1.png","images/problems/c9_2.png","images/problems/c9_3.png","images/problems/c9_4.png","images/problems/c9_5.png","images/problems/c9_6.png","images/problems/c9_7.png","components/ChapterButton.js","components/Footer.js","components/TopPage.js","components/SelectLabel.js","components/problems/c1.js","components/problems/c2.js","components/problems/c3.js","components/problems/c4.js","components/problems/c5.js","components/problems/c6.js","components/problems/c7.js","components/problems/c8.js","components/problems/c9.js","components/problems/c10.js","components/ProblemPage.js","components/ErrorPage.js","components/App.js","index.js"],"names":["module","exports","__webpack_require__","p","map","./c10_1.png","./c10_9.png","./c1_1.png","./c2_1.png","./c2_2.png","./c2_3.png","./c3_1.png","./c3_2.png","./c3_3.png","./c3_4.png","./c3_5.png","./c3_6.png","./c3_7.png","./c4_1.png","./c4_2.png","./c4_3.png","./c4_4.png","./c4_5.png","./c5_1.png","./c5_2.png","./c5_3.png","./c6_1.png","./c6_2.png","./c6_3.png","./c6_4.png","./c6_5.png","./c7_1.png","./c7_2.png","./c7_3.png","./c7_4.png","./c7_5.png","./c8_1.png","./c8_2.png","./c8_3.png","./c9_1.png","./c9_2.png","./c9_3.png","./c9_4.png","./c9_5.png","./c9_6.png","./c9_7.png","webpackContext","req","id","webpackContextResolve","e","Error","code","keys","Object","resolve","ChapterButton","props","react_default","a","createElement","className","react_router_dom","style","textDecoration","color","to","concat","value","src","isClear","check_green_path","check_gray_path","alt","title","Footer","TopPage","titleList","data","_React$useState","React","useState","_userLog","clearCnt","clearList","sumCorrect","i","_clearList","JSON","parse","localStorage","getItem","_allCorrect","k","length","push","_React$useState2","slicedToArray","userLog","chapterButtonDom","key","clickHandler","href","target","cover_path","background","Footer_Footer","SelectLable","lazyCheckList","checkList","answer","handleChange","text","cls","type","name","checked","onChange","disabled","undefined","display","dangerouslySetInnerHTML","__html","problems","question","selection","img","c1","c2","c3","c4","c5","c6","c7","c8","c9","all_problems","ProblemPage","chapter","p_base","p_length","checkListInit","isLastChapter","initClearList","Array","problemClearList","setProblemClearList","_React$useState3","_React$useState4","setIsClear","_React$useState5","_React$useState6","isEnd","setIsEnd","_React$useState7","_React$useState8","correctCnt","setCorrectCnt","_React$useState9","_React$useState10","setCheckList","_React$useState11","_React$useState12","setLazyCheckList","_useReward","useReward","position","reward","isAnimating","event","_event$target","oldData","objectSpread","defineProperty","SeleclLabels","problemDomList","_loop","p_key","q","selection_list","selectionDom","index","String","container_cls","problemDom","check_green_light_path","check_wrong_path","require","useEffect","setTimeout","back_on_path","onSubmit","preventDefault","allCorrect","cnt","yourCheckList","isCorrect","setItem","stringify","checkClear","onClick","ErrorPage","App","pages","react_router","path","element","ProblemPage_ProblemPage","basename","process","TopPage_TopPage","ErrorPage_ErrorPage","ReactDOM","createRoot","document","getElementById","render","App_App"],"mappings":"4EAAAA,EAAAC,QAAA,4iECAAD,EAAAC,QAAA,yjECAAD,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAA,o/DCAAD,EAAAC,QAAA,w3DCAAD,EAAAC,QAAA,moECAA,IAAAG,EAAA,CACAC,cAAA,GACAC,cAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,GACAC,aAAA,IAIA,SAAAC,EAAAC,GACA,IAAAC,EAAAC,EAAAF,GACA,OAAA7C,EAAA8C,GAEA,SAAAC,EAAAF,GACA,IAAAC,EAAA5C,EAAA2C,GACA,KAAAC,EAAA,IACA,IAAAE,EAAA,IAAAC,MAAA,uBAAAJ,EAAA,KAEA,MADAG,EAAAE,KAAA,mBACAF,EAEA,OAAAF,EAEAF,EAAAO,KAAA,WACA,OAAAC,OAAAD,KAAAjD,IAEA0C,EAAAS,QAAAN,EACAjD,EAAAC,QAAA6C,EACAA,EAAAE,GAAA,oBC/DAhD,EAAAC,QAAiBC,EAAAC,EAAuB,mDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,mDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,kDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,qMCKzB,SAASqD,EAAcC,GAClC,OAEIC,EAAAC,EAAAC,cAAA,OAAKC,UAAU,gBACXH,EAAAC,EAAAC,cAACE,EAAA,EAAD,CAAMD,UAAU,yBAAyBE,MAAO,CAAEC,eAAgB,OAAQC,MAAO,WAAYC,GAAE,WAAAC,OAAaV,EAAMW,MAAnB,MAC3FV,EAAAC,EAAAC,cAAA,OAAKS,IAAKZ,EAAMa,QAAUC,IAAmBC,IAAiBX,UAAU,iBAAiBY,IAAI,iBAE7Ff,EAAAC,EAAAC,cAAA,KAAGC,UAAU,eAAeJ,EAAMiB,eCTnC,SAASC,IACpB,OACIjB,EAAAC,EAAAC,cAAA,OAAKC,UAAU,oBAEXH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,cAAb,yECAG,SAASe,EAAQnB,GAkC5B,IAjCA,IAEMoB,EAAYpB,EAAMoB,UAAUC,KAHCC,EAKdC,IAAMC,SACvB,WAOI,IANA,IAAMC,EAAW,CACbC,SAAS,EACTC,UAAU,IAEVC,EAAa,EAETC,EAAE,EAAGA,EAZF,GAYiBA,IAAI,CAI5B,IAHA,IAAMC,EAAaC,KAAKC,MAAMC,aAAaC,QAAb,gBAAAxB,OAAqCmB,EAAE,EAAvC,cAAuD,EAAC,GAElFM,GAAc,EACVC,EAAE,EAAGA,EAAIN,EAAWO,OAAQD,IAC5BN,EAAWM,GACXR,GAAc,EAEdO,GAAc,EAItBV,EAASE,UAAUW,KAAKH,GAI5B,OADAV,EAASC,SAAWE,EACbH,IA7BoBc,EAAA1C,OAAA2C,EAAA,EAAA3C,CAAAyB,EAAA,GAK5BmB,EAL4BF,EAAA,GAiC7BG,GAjC6BH,EAAA,GAiCV,IACjBV,EAAE,EAAGA,EAjCM,GAiCSA,IACxBa,EAAiBJ,KACjBrC,EAAAC,EAAAC,cAACJ,EAAD,CAAe4C,IAAKd,EAAGhB,QAAS4B,EAAQd,UAAUE,GAAIlB,MAAOkB,EAAE,EAAGZ,MAAOG,EAAUS,GAAIe,aAAc5C,EAAM4C,gBAI/G,OACI3C,EAAAC,EAAAC,cAAA,OAAKC,UAAU,kBACXH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,qBACXH,EAAAC,EAAAC,cAAA,KAAG0C,KAAK,gFAAgFC,OAAO,UAC3F7C,EAAAC,EAAAC,cAAA,OAAKS,IAAKmC,IAAY3C,UAAU,YAAYY,IAAI,gBAEpDf,EAAAC,EAAAC,cAAA,MAAIC,UAAU,aAAd,qCACAH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,gBAAb,6GACAH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAb,kCAEAH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,2BACfH,EAAAC,EAAAC,cAAA,MAAIG,MAAO,CAAC0C,WAAU,kEAAAtC,OAAoE+B,EAAQf,SAA5E,aAAAhB,OAAgG+B,EAAQf,SAAxG,kBAAkItB,UAAU,iBAC9JH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,oBAAhB,sBACAH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,kBAAkBqC,EAAQf,UAC1CzB,EAAAC,EAAAC,cAAA,QAAMC,UAAU,oBAAhB,kBAQRH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,yBACNsC,IAKbzC,EAAAC,EAAAC,cAAC8C,EAAD,qCCvEG,SAASC,EAAYlD,GAAO,IAChCmD,EAAmEnD,EAAnEmD,cAAeC,EAAoDpD,EAApDoD,UAAWC,EAAyCrD,EAAzCqD,OAAQC,EAAiCtD,EAAjCsD,aAAcC,EAAmBvD,EAAnBuD,KAAMhE,EAAaS,EAAbT,GAAIoB,EAASX,EAATW,MAG7D6C,EAAM,UAeV,OAdIL,EAE2B,KAAtBA,EAAc5D,GACnBiE,GAAO,GACFH,EAAO9D,KAAQoB,EACpB6C,GAAO,aACDL,EAAc5D,KAAQ8D,EAAO9D,IAAQ4D,EAAc5D,KAAQoB,EACjE6C,GAAO,aACFL,EAAc5D,KAAQ8D,EAAO9D,IAAO4D,EAAc5D,KAAQoB,EAC/D6C,GAAO,mBAEPA,GAAO,WAVPA,GAAO,GAcPvD,EAAAC,EAAAC,cAAA,OAAKC,UAAWoD,GACZvD,EAAAC,EAAAC,cAAA,SAAOC,UAAU,mBACbH,EAAAC,EAAAC,cAAA,SACAsD,KAAK,QACLC,KAAMnE,EACNa,UAAU,cACVO,MAAOA,EACPgD,QAASP,EAAU7D,KAAQoB,EAC3BiD,SAAUN,EAEVO,cAA0BC,IAAhBX,IAEVlD,EAAAC,EAAAC,cAAA,OAAKC,UAAU,sBAAsBE,MAAO,CAACyD,QAAQ,UAAUC,wBAAyB,CAACC,OAAQV,OCuEjH,mEA3GMW,EAAW,CACb,CACIC,SAAU,+LACVC,UAAW,CACP,6FACA,iFACA,0GAEJC,IAAK,KACLhB,OAAQ,KAEZ,CACIc,SAAU,+YACVC,UAAW,CACP,2KACA,yJACA,sKAEJC,IAAK,KACLhB,OAAQ,KAEZ,CACIc,SAAU,6OACVC,UAAW,CACP,iIACA,iIACA,kIAEJC,IAAK,WACLhB,OAAQ,KAEZ,CACIc,SAAU,wbACVC,UAAW,CACP,6EACA,iEACA,8EAEJC,IAAK,KACLhB,OAAQ,KAEZ,CACIc,SAAU,ulBACVC,UAAW,CACP,uLACA,oDACA,8RAEJC,IAAK,KACLhB,OAAQ,KAEZ,CACIc,SAAU,ilBACVC,UAAW,CACP,uZACA,wVACA,2VAEJC,IAAK,KACLhB,OAAQ,KAEZ,CACIc,SAAU,8gBACVC,UAAW,CACP,6IACA,yJACA,0JAEJC,IAAK,GACLhB,OAAQ,KAEZ,CACIc,SAAU,+IACVC,UAAW,CACP,mGACA,6IACA,iFACA,8IAEJC,IAAK,GACLhB,OAAQ,KAEZ,CACIc,SAAU,qdACVC,UAAW,CACP,mJACA,6IACA,kIAEJC,IAAK,GACLhB,OAAQ,KAEZ,CACIc,SAAU,2iBACVC,UAAW,CACP,uIACA,+JACA,8IAGJC,IAAK,GACLhB,OAAQ,MAKV3G,EAAI,GACFmF,EAAE,EAAGA,EAAIqC,EAAS7B,OAAQR,IAAInF,EAAC,IAAAgE,OAAKmB,EAAE,IAAOqC,EAASrC,GCS9D,IDRO,IAAMyC,EAAK5H,EC5GZwH,EAAW,CAEb,CACIC,SAAU,4KACVC,UAAW,CACP,oLACA,oUACA,qIAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,gMACVC,UAAW,CACP,6SACA,4SACA,8SAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,kPACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,qrBACVC,UAAW,CACP,6IACA,uIACA,wIAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,mRACVC,UAAW,CACP,0WAEA,sHAEA,uTAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,yHACVC,UAAW,CACP,mJACA,yJACA,oPAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,geACVC,UAAW,CACP,6LACA,2KACA,wLAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,uMACVC,UAAW,CACP,+KACA,8YACA,0GAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,giCACVC,UAAW,CACP,wKACA,iRACA,2ZAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,mSACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,MAMV3G,EAAI,GACFmF,EAAE,EAAGA,EAAIqC,EAAS7B,OAAQR,IAAInF,EAAC,IAAAgE,OAAKmB,EAAE,IAAOqC,EAASrC,GCA9D,IDCO,IAAM0C,EAAK7H,ECrHZwH,EAAW,CAEb,CACIC,SAAU,gzBACVC,UAAW,CACP,8OAEA,8OAEA,qLAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,yKACVC,UAAW,CACP,2HACA,qWACA,+mBAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,giBACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,mKACVC,UAAW,CACP,sXACA,mPACA,yPAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,qZACVC,UAAW,CACP,2KACA,6IACA,4KAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,iRACVC,UAAW,CACP,4EACA,sVACA,qTAGJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,2GACVC,UAAW,CACP,yHACA,yHACA,wHAGJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,+HACVC,UAAW,CACP,+JACA,sWACA,mMAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,6iBACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,6cACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,MAKV3G,EAAI,GACFmF,EAAE,EAAGA,EAAIqC,EAAS7B,OAAQR,IAAInF,EAAC,IAAAgE,OAAKmB,EAAE,IAAOqC,EAASrC,GCC9D,IDAO,IAAM2C,EAAK9H,ECrHZwH,EAAW,CAEb,CACIC,SAAU,u5BACVC,UAAW,CACP,qKACA,qKACA,0JAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8MACVC,UAAW,CACP,qTAEA,wZAEA,0HAGJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,44BACVC,UAAW,CACP,+HACA,yHACA,kJAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,miBACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,iXACVC,UAAW,CACP,8OACA,gNACA,qPAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,+gBACVC,UAAW,CACP,gIACA,iIACA,oYAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,mfACVC,UAAW,CACP,qIACA,+HACA,yGAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8TACVC,UAAW,CACP,2KACA,+JACA,iKAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,+VACVC,UAAW,CACP,iCACA,uCACA,wCAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,2LACVC,UAAW,CACP,iLACA,yJACA,yJAEJC,IAAK,WACLhB,OAAQ,MAKV3G,EAAI,GACFmF,EAAE,EAAGA,EAAIqC,EAAS7B,OAAQR,IAAInF,EAAC,IAAAgE,OAAKmB,EAAE,IAAOqC,EAASrC,GCH9D,IDIO,IAAM4C,EAAK/H,ECtHZwH,EAAW,CAEb,CACIC,SAAU,qdACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,yiBACVC,UAAW,CACP,qFACA,mHACA,0HAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,mcACVC,UAAW,CACP,qIACA,qIACA,0HAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,idACVC,UAAW,CACP,iOACA,+NACA,oOAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,+gBACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,qoCACVC,UAAW,CACP,iLACA,uJACA,4QAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,qUACVC,UAAW,CACP,iLACA,uJACA,4QAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,yRACVC,UAAW,CACP,sWACA,gKACA,kUAGJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,kkBACVC,UAAW,CACP,kEACA,8BACA,yIAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,2sBACVC,UAAW,CACP,mMACA,sKACA,wLAEJC,IAAK,GACLhB,OAAQ,MAKV3G,EAAI,GACFmF,EAAE,EAAGA,EAAIqC,EAAS7B,OAAQR,IAAInF,EAAC,IAAAgE,OAAKmB,EAAE,IAAOqC,EAASrC,GCE9D,IDDO,IAAM6C,EAAKhI,ECnHZwH,EAAW,CAEb,CACIC,SAAU,uPACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,sLACVC,UAAW,CACP,oDACA,wQACA,qDAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,wTACVC,UAAW,CACP,+EACA,+HACA,oHAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,gHACVC,UAAW,CACP,mGACA,wKACA,6OAEJC,IAAK,GACLhB,OAAQ,KAKZ,CACIc,SAAU,8OACVC,UAAW,CACP,iOACA,wRACA,6QAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,0VACVC,UAAW,CACP,iLACA,mLACA,8LAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,uIACVC,UAAW,CACP,6CACA,yDACA,2EAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,uRACVC,UAAW,CACP,WACA,gBACA,SAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,uIACVC,UAAW,CACP,6CACA,yDACA,mFAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,qTACVC,UAAW,CACP,WACA,gBACA,SAEJC,IAAK,WACLhB,OAAQ,MAKV3G,EAAI,GACFmF,EAAE,EAAGA,EAAIqC,EAAS7B,OAAQR,IAAInF,EAAC,IAAAgE,OAAKmB,EAAE,IAAOqC,EAASrC,GCC9D,IDAO,IAAM8C,EAAKjI,ECrHZwH,EAAW,CAEb,CACIC,SAAU,8EACVC,UAAW,CACP,6IACA,yJACA,oJAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,0IACVC,UAAW,CACP,iCACA,iCACA,kCAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,q1BACVC,UAAW,CACP,+JACA,yJACA,8IAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,siBACVC,UAAW,CACP,iGACA,iGACA,kGAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,2MACVC,UAAW,CACP,2BACA,2BACA,kCAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,6NACVC,UAAW,CACP,2NACA,uIACA,oJAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,yIACVC,UAAW,CACP,6JACA,yJACA,uNAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,iQACVC,UAAW,CACP,iLACA,+MACA,wOAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,kRACVC,UAAW,CACP,qCACA,sCAGJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,0fACVC,UAAW,CACP,wOAEA,4XAEA,0fAEJC,IAAK,WACLhB,OAAQ,MAKV3G,EAAI,GACFmF,GAAE,EAAGA,GAAIqC,EAAS7B,OAAQR,KAAInF,EAAC,IAAAgE,OAAKmB,GAAE,IAAOqC,EAASrC,ICF9D,IDGO,IAAM+C,GAAKlI,ECtHZwH,GAAW,CAEb,CACIC,SAAU,sTACVC,UAAW,CACP,+DACA,uCACA,wCAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,6NACVC,UAAW,CACP,yXACA,6LACA,8LAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,skBACVC,UAAW,CACP,qJACA,uLACA,qKAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,iKACVC,UAAW,CACP,2KACA,sNACA,4NAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,6fACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,2MACVC,UAAW,CACP,iTACA,0JACA,2TAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8pBACVC,UAAW,CACP,0HACA,uHACA,iLAGJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,yIACVC,UAAW,CACP,gKACA,ySACA,0PAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,gQACVC,UAAW,CACP,yDACA,uCACA,8CAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,orBACVC,UAAW,CACP,qIACA,gIACA,+GAEJC,IAAK,WACLhB,OAAQ,MAKV3G,GAAI,GACFmF,GAAE,EAAGA,GAAIqC,GAAS7B,OAAQR,KAAInF,GAAC,IAAAgE,OAAKmB,GAAE,IAAOqC,GAASrC,ICE9D,IDDO,IAAMgD,GAAKnI,GCpHZwH,GAAW,CAEb,CACIC,SAAU,keACVC,UAAW,CACP,6JACA,6JACA,8JAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,6RACVC,UAAW,CACP,iCACA,iCACA,YAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,iTACVC,UAAW,CACP,kQACA,6TACA,oXAGJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,gcACVC,UAAW,CACP,2KACA,uHACA,4KAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,kYACVC,UAAW,CACP,wRACA,qMACA,qaAGJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,wcACVC,UAAW,CACP,kFACA,qFACA,yFAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,2gCACVC,UAAW,CACP,qBACA,uCACA,kCAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,0hBACVC,UAAW,CACP,MACA,eACA,aAEJC,IAAK,WACLhB,OAAQ,KAGZ,CACIc,SAAU,giBACVC,UAAW,CACP,oCACA,8BAEJC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,uQACVC,UAAW,CACP,qHACA,iFACA,gHAGJC,IAAK,GACLhB,OAAQ,MAKV3G,GAAI,GACFmF,GAAE,EAAGA,GAAIqC,GAAS7B,OAAQR,KAAInF,GAAC,IAAAgE,OAAKmB,GAAE,IAAOqC,GAASrC,ICpB9D,IDqBO,IAAMiD,GAAKpI,GCtHZwH,GAAW,CAEb,CACIC,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,8QACVC,UAAW,GAGXC,IAAK,GACLhB,OAAQ,KAGZ,CACIc,SAAU,6JACVC,UAAW,CACP,iFACA,uFACA,4EAEJC,IAAK,YACLhB,OAAQ,MAKV3G,GAAI,GACFmF,GAAE,EAAGA,GAAIqC,GAAS7B,OAAQR,KAAInF,GAAC,IAAAgE,OAAKmB,GAAE,IAAOqC,GAASrC,IACvD,ICzEDkD,GAAe,CAACT,EAAIC,EAAIC,EAAIC,EAAIC,EAAIC,EAAIC,GAAIC,GAAIC,GDyEnCpI,ICvEJ,SAASsI,GAAYhF,GAqBhC,IAnBA,IAAMiF,EAAUjF,EAAMiF,QAEhBC,EAASH,GAAaE,EAAQ,GAC9BE,EAAWtF,OAAOD,KAAKsF,GAAQ7C,OAC/BgB,EAAS,GACT+B,EAAgB,GAGhBnE,EAAQjB,EAAMiB,MACdoE,EAFa,KAEGJ,EAXiB3D,EAaSC,IAAMC,SAAS,WAE3D,IADA,IAAM8D,EAAgB,IAAIC,MAClB1D,EAAE,EAAGA,EAAGsD,EAAUtD,IACtByD,EAAchD,MAAK,GAEvB,OAAOP,KAAKC,MAAMC,aAAaC,QAAb,gBAAAxB,OAAqCuE,EAArC,cAA2DK,IAlB1C/C,EAAA1C,OAAA2C,EAAA,EAAA3C,CAAAyB,EAAA,GAahCkE,EAbgCjD,EAAA,GAadkD,EAbclD,EAAA,GAqB/BV,EAAE,EAAGA,EAAG2D,EAAiBnD,OAAQR,IACrC,IAAI2D,EAAiB3D,GAAG,EACP,EACb,MAxB+B,IAAA6D,EA4BTnE,IAAMC,UAAS,GA5BNmE,EAAA9F,OAAA2C,EAAA,EAAA3C,CAAA6F,EAAA,GA4BhC7E,EA5BgC8E,EAAA,GA4BvBC,EA5BuBD,EAAA,GAAAE,EA8BbtE,IAAMC,UAAS,GA9BFsE,EAAAjG,OAAA2C,EAAA,EAAA3C,CAAAgG,EAAA,GA8BhCE,EA9BgCD,EAAA,GA8BzBE,EA9ByBF,EAAA,GAAAG,EA+BH1E,IAAMC,SAAS,GA/BZ0E,EAAArG,OAAA2C,EAAA,EAAA3C,CAAAoG,EAAA,GA+BhCE,EA/BgCD,EAAA,GA+BpBE,EA/BoBF,EAAA,GAAAG,EAgCL9E,IAAMC,SAAS4D,GAhCVkB,EAAAzG,OAAA2C,EAAA,EAAA3C,CAAAwG,EAAA,GAgChCjD,EAhCgCkD,EAAA,GAgCrBC,EAhCqBD,EAAA,GAAAE,EAiCGjF,IAAMC,cAASsC,GAjClB2C,EAAA5G,OAAA2C,EAAA,EAAA3C,CAAA2G,EAAA,GAiChCrD,EAjCgCsD,EAAA,GAiCjBC,EAjCiBD,EAAA,GAAAE,EAmCPC,YAAU,WAAY,WAAY,CAE9DC,SAAU,aAFNC,EAnC+BH,EAmC/BG,OAnC+BH,EAmCvBI,YA6ChB,SAASzD,EAAa0D,GAAO,IAAAC,EACHD,EAAMlE,OAArBY,EADkBuD,EAClBvD,KAAM/C,EADYsG,EACZtG,MACb4F,EAAa,SAAAW,GACT,OAAOrH,OAAAsH,EAAA,EAAAtH,CAAA,GAAIqH,EAAXrH,OAAAuH,EAAA,EAAAvH,CAAA,GACK6D,EAAO/C,MAoBpB,SAAS0G,IAGL,IAFA,IAAMC,EAAiB,GADJC,EAAA,SAGX1F,GACJ,IAAM2F,EAAK,IAAA9G,OAAOmB,GACZ4F,EAAIvC,EAAOsC,GAEXE,EAAiBD,EAAErD,UACzBf,EAAOmE,GAASC,EAAEpE,OAClB+B,EAAcoC,GAAS,GAEvB,IAAMG,EAAeD,EAAe/K,IAAI,SAACgE,EAAOiH,GAC5C,OAAO3H,EAAAC,EAAAC,cAAC+C,EAAD,CACHE,UAAWA,EACXD,cAAeA,EACfE,OAAQA,EACRC,aAAcA,EACdC,KAAM5C,EACNpB,GAAIiI,EACJ7E,IAAG,GAAAjC,OAAK8G,EAAL,KAAA9G,OAAcmB,EAAd,KAAAnB,OAAmBkH,GACtBjH,MAAOkH,OAAOD,EAAM,OAIxBE,EAAgB,gBACjB3E,GAA0C,KAAzBA,EAAcqE,KAC9BM,GAAiB,kBAGrB,IAAMC,EAAa9H,EAAAC,EAAAC,cAAA,OAAKZ,GAAE,IAAAmB,OAAMmB,GAAKc,IAAG,GAAAjC,OAAK8G,EAAL,KAAA9G,OAAcmB,GAAKzB,UAAW0H,GAC9D7H,EAAAC,EAAAC,cAAA,OAAKC,UAAU,uBACXH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,uBACT2F,GAAS9F,EAAAC,EAAAC,cAAA,OAAKS,IAAK4E,EAAiB3D,EAAE,GAAMf,IAAmBC,IAAiBX,UAAU,iBAAiBY,IAAI,iBAEjHf,EAAAC,EAAAC,cAAA,MAAIC,UAAU,oBAAd,gBAAqCyB,IAExCkE,GACG9F,EAAAC,EAAAC,cAAA,OAAKS,IAAKwC,EAAS,IAAA1C,OAAKmB,MAASwB,EAAM,IAAA3C,OAAKmB,IAAOmG,IAAyBC,IAAkB7H,UAAU,mBAAmBY,IAAI,kBAGvIf,EAAAC,EAAAC,cAAA,KAAGC,UAAU,mBACTH,EAAAC,EAAAC,cAAA,QAAM6D,wBAAyB,CAACC,OAAQwD,EAAEtD,aAE7CsD,EAAEpD,KAAOpE,EAAAC,EAAAC,cAAA,OAAKC,UAAU,QAAQQ,IAAKsH,MAAQ,KAAAxH,OAAsB+G,EAAEpD,QACrEsD,GAGTL,EAAehF,KAAKyF,IA5ChBlG,EAAE,EAAGA,EAAIsD,EAAS,EAAGtD,IAAK0F,EAA1B1F,GA8CR,OAAOyF,EAGX,OA7DA/F,IAAM4G,UAAU,WACRtH,IAEAuH,WAAWtB,EAAQ,KACnBsB,WAAWtB,EAAQ,QAExB,CAACjG,IAwDAZ,EAAAC,EAAAC,cAAA,OAAKC,UAAU,eAEXH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,sBAGXH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,WAAWa,IAE7BhB,EAAAC,EAAAC,cAACE,EAAA,EAAD,CAAMI,GAAI,KACNR,EAAAC,EAAAC,cAAA,OAAKS,IAAKyH,IAAcjI,UAAU,cAM1CH,EAAAC,EAAAC,cAAA,QAAMmI,SAnGd,SAAkBtB,GACdA,EAAMuB,iBACN7B,EAAiBtD,GACjB4C,GAAS,GApCb,WAKI,IAJA,IAAIwC,GAAa,EACbC,EAAM,EACJC,EAAgB,IAAInD,MAElB1D,EAAE,EAAGA,EAAEsD,EAAUtD,IAAI,CACzB,IAAMc,EAAG,IAAAjC,OAAOmB,EAAE,GAEd8G,GAAY,EACbvF,EAAUT,KAASU,EAAOV,IACzB8F,GAAO,EACPE,GAAY,GAEZH,GAAa,EAIdhD,EAAiB3D,KAChB8G,GAAY,GAEhBD,EAAcpG,KAAKqG,GAGvBlD,EAAoBiD,GACpBzG,aAAa2G,QAAb,gBAAAlI,OAAqCuE,EAArC,WAAuDlD,KAAK8G,UAAUH,IAEtEtC,EAAcqC,GACXD,GACC5C,GAAW,GASfkD,KAgGQ7I,EAAAC,EAAAC,cAACkH,EAAD,MACApH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,kBACT2F,GAAS9F,EAAAC,EAAAC,cAAA,UAAQC,UAAU,MAAMb,GAAG,cAA3B,mCAGfU,EAAAC,EAAAC,cAAA,OAAKC,UAAU,qBACT2F,GAAS9F,EAAAC,EAAAC,cAAA,QAAMZ,GAAG,aACnBwG,GAAS9F,EAAAC,EAAAC,cAAA,MAAIZ,GAAG,WAAWa,UAAU,cAClCH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,oBAAhB,gBACAH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAc+F,GAC9BlG,EAAAC,EAAAC,cAAA,QAAMC,UAAU,oBAAhB,KAAAM,OAAyCyE,IACxCtE,GAAWZ,EAAAC,EAAAC,cAAA,QAAMC,UAAU,aAAa2I,QAASjC,GAAtC,kBAGdf,GAASlF,GAAWwE,GAClBpF,EAAAC,EAAAC,cAACE,EAAA,EAAD,CAAMI,GAAE,KACRR,EAAAC,EAAAC,cAAA,UAAQC,UAAU,kBAAlB,uBAGF2F,GAASlF,IAAYwE,GACnBpF,EAAAC,EAAAC,cAACE,EAAA,EAAD,CAAMI,GAAE,WAAAC,OAAauE,EAAU,EAAvB,MACRhF,EAAAC,EAAAC,cAAA,UAAQC,UAAU,kBAAlB,mCAIF2F,IAAUlF,GAAYZ,EAAAC,EAAAC,cAAA,UAAQ4I,QA7GhD,WACI/C,GAAS,GACTU,OAAiB5C,GACjByC,EAAanB,IA0G+ChF,UAAU,kBAAlC,8BAIhCH,EAAAC,EAAAC,cAAC8C,EAAD,aCjOG,SAAS+F,KACpB,OACI/I,EAAAC,EAAAC,cAAA,OAAKC,UAAU,mBACXH,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,2BAEAF,EAAAC,EAAAC,cAACE,EAAA,EAAD,CAAMI,GAAI,KACVR,EAAAC,EAAAC,cAAA,UAAQC,UAAU,kBAAlB,sBCHG,SAAS6I,KAiBtB,IAhBE,IAAM7H,EAAW,CACfC,KAAM,CACN,2DACA,2DACA,qDACA,yCACA,qDACA,mBACA,8EACA,cACA,yCACA,4CAIE6H,EAAQ,GACNrH,EAAE,EAAGA,EAAGT,EAAUC,KAAKgB,OAAQR,IAAI,CACzC,IAAMnF,EAAIuD,EAAAC,EAAAC,cAACgJ,GAAA,EAAD,CAAOxG,IAAKd,EAAGuH,KAAI,WAAA1I,OAAamB,EAAE,EAAf,KAAqBwH,QAASpJ,EAAAC,EAAAC,cAACmJ,GAAD,CAAarE,QAASpD,EAAE,EAAGZ,MAAOG,EAAUC,KAAKQ,OAC5GqH,EAAM5G,KAAK5F,GAGb,OACEuD,EAAAC,EAAAC,cAACE,EAAA,EAAD,CAAekJ,SAAUC,gBACvBvJ,EAAAC,EAAAC,cAACgJ,GAAA,EAAD,KACElJ,EAAAC,EAAAC,cAACgJ,GAAA,EAAD,CAAOC,KAAI,IAAOC,QAASpJ,EAAAC,EAAAC,cAACsJ,EAAD,CAASrI,UAAWA,MAC9C8H,EACDjJ,EAAAC,EAAAC,cAACgJ,GAAA,EAAD,CAAOC,KAAK,IAAIC,QAASpJ,EAAAC,EAAAC,cAACuJ,GAAD,UC9BpBC,IAASC,WAAWC,SAASC,eAAe,SACpDC,OAED9J,EAAAC,EAAAC,cAAC6J,GAAD","file":"static/js/main.5fef60a5.chunk.js","sourcesContent":["module.exports = \"data:image/png;base64,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\"","module.exports = \"data:image/png;base64,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\"","module.exports = __webpack_public_path__ + \"static/media/cover.4518cb75.jpg\";","module.exports = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEEAAABBCAYAAACO98lFAAAACXBIWXMAAAsSAAALEgHS3X78AAAFiUlEQVR4nNVcwW7bOBCdzd43hs4C7It6TfYL6j+oz3uJ+wFF0y+o+wXrfME6p7062B+wj3va+KzD2oDOQvwBRQq6Ty5NDSmSGqn2AwwjUkKRTzPDmeFMfnl9faU+UKbZLRGNiEh9D/DNYYVr6nubFPm26+l1RkKZZmrBEyIa43MdOdQOhCzVd1LkL8JTlSehTLMpEanPW9GBv2MPMhZJka98/sAHYpKAxc+IaCg1uQas1fMkyGhNQsTiN0SkRPoZ3zpG2sd3PEXGfVLkz3EraEECdH7hIfYbTZ+931qZZgPNnkw8SHmAZATbjCgSyjSbgACbsVO6O4fuilh37C73RHTn+DVF+DRUKoJJKNNMLe6j5fYOb2MRNGjY8wcg47PlV/ZQD+85BJFQptnC8Sa+qLffxRZmmcsI0vbO8ivvfYnwIgHsK32+YW5HiaAUGlTzMSnyadOjrjznsrAQ8KgM188iQCEp8iW8zw1z+65Ms/umMRolwaECn5Iin4dMuEtAWueWuTpVw0kCWPwzdNCfCcdL+90msVYSsCX9d0kEVLAQoXauW85wszYBorVkbj2cOwH03U5MGRsxhG2rwWYY7xkPbZ0UeaOROSOM4TPoeFem2biRBOy/piOyh+t6MYDYc3OuSQMnCTPm2rQvJ0gSiFUejCGHCPqOODGMkIL/jT9SalAToUsB7NvWcKZ2SZGPqh9MSWClwHe9ZZrNyjR7xecZExCB2q3KNHvRxvd6MZBgc10n0nAkARM2t5XHFlGg8jBXEkRgu17Fpujg1JlG8mjkdUnwMiKBaE1EWwI0mN7tDdT/hART7DcRqas5sz9HE+Eg4DFibtwLPbz4gyRggmaGKFgKoH9jCSIaCPC2U9rclFo/GZcP41TqwBkZzmP0eVhrIqQJ0GCuSanEwEbCrk1arA0RHRJA2sGOjtuKBPM0qHUaO4aIjgmoVGJnXB7bSBBJkoQQ0TUBGkwJP0qC+WCxTJEPET0SQIyUD64wAROicUIDEf/iAKUPAog78FHqUNPNLnKGDiLeENFvxrWuCCBGyoe+iVYROIjQ0SUBLHolgX4Q8YGIvjK3S92n7wu9kwAb9A8R/crcTqSCrhD0SoJnMCQWfVpQG1eRUPMMfWP1EDgI+Fsy6PKAuRturvqoCWrwA/6QCro8YY73UqlDzZWUeqKPIyQZfXrAlIRtRYK5d9oqy4IQ4gn2SEQtRLCR0FoSYlzhronAnGohQkWC6U9fW9zpkIdFxQIdE1FLIaoM1YEEpKrMRGSU1yYRDHVIhEnCIdOk+wlm1iWYBMloUJoIzM2ssTis2UXCtXlS44GJZDTYQESounLu+CkJqPiw5uYj0ToY8gy6nEBqnTtTeSHGbeZy8yEHsVvkBtY4xheJBjUinrTxQ3Ie3MnaMZtunkU2nttdGnzOV08kAYyb0qDO7TgmLwXc+cnJemrlOhZpIFfNz7nCUnNVO2WvhdKQBtaS9h3ntwG2RJOAPbf1s/kE1CWtjctDifOIPgA7wM11xkXNrqTKhNkyb1AZds4EVEVnpjqvbXWXVhIcNT9350qEo/x456q58qloVTr0F3NLqcvkXGqZYAOWTNXdvqn0uDHHCPvwnrn1Fj68SO6hDeDQrWIIoJBSf4dE7GFweq9zhvjPLP0XXgRQRL+DjQiSbMwKmMvckrn2JoAiO1/GFutboVMyPBrPNrBV3gnk2B4onyawDd7Usq3xxPOqfktXQ1hUM1irlkC4pTOPyrKqI06J53PTW4KxrT4+nXA7VN1GSZ9EX+QIRLi61DjsuIKJwFK9quuuVe+VZIdsLBkxEFl8BfGGcWxblf5yfVNt8AQbI+qxdvqvA4zO+duIPuoN7MhKwsDa0Nv/T6igHfZyBzzHHurechdE9A2d/Q86Z/0ALwAAAABJRU5ErkJggg==\"","module.exports = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEEAAABACAYAAABFqxrgAAAACXBIWXMAAAsSAAALEgHS3X78AAAFLElEQVR4nOVcTXLaMBR+zXTv3IDcgKy9CWu8CD1ByAniniD0BCUnKJygsMBr2LCGG8AN4hOkI/KZys9Psmz8l+Gb8UxjC0v6/P70ntRvHx8fVCf8KLgjontcAyJSf/dyutwQ0YGIdkS03g5XuzrHWAsJmPiIiMZE1K/glTERLdS1Ha4WFbwvhUpJ8KNgjIk/VPbSLBQhMyKaboerQxUvrIQETH7iIOZVY676vZSMi0jwo0Dp+dTxy++h42rAa0u7O1wDx/fGkIpJgaGnUJoEPwpUp685zZbQ5XXZr+VHwQD2ZZQjaYrkcRkjWpgEPwpu8SVNBi+GdMyq0lmtb0VEaJEQ1Xe4Ha5mRd5biASIvyLAMwxgCtF8LzKIooB0TCxkzLfD1dj1tc4koOOFgYAlvkClX95hTCHIkMbkTIQTCRDDv8KjGNZ56tJZHUBMsjCopxMRuSRYVEARMKg7mnOFHwXKDjwJzXOJuLE9BMsSAcoS33WFAAVM9Fl49ORHgVVSjZJg8QJ7SECtxq8sELj9EX7+wxRy2yRhIhBw7DIB9CkRSi1+Co9mkOwMRBLgCV7YbWUDRl0mIAEM9Zzd9rDmyMAkCVLjUtFYiwihujoeoC4pZEhAOMzD03kdS9g6AYmVvMIU9u6MFAl4GLIfxcK9LwFI7i82Vo/P5ztrEAruMOywJ+AfLbNeUatLqIAu3UmkeQJXBy4+x6KLkaagufBX7RKtvz5hwNNtw5kEhMbcFpReozcA20o2BXzII7t9liBdErgUxB2WgpmBAJva8qixn8QNOgmPrFGXCZDWCM85Llyaj5L+TxKgCi4/ahU5BFjHC+O+NJKAmoCOY9cCo0sI0MBzm6ekTELCIKdxq6iIAHFeaolgkoTOSIGFgLeihhvSHbPb9zfwtzxA6kqiJLQkSspGsXxut0oSuBRIDRsHgpnfBgKck6gCeB70rA4ptB0mWxIjlxJAAgn29FobqJkAEWoBxT0DX4OngHxdIim7KpfYbRBAwirSBfda0SNWLqaKmMJCwLJOAgjqwH1n3qJET0gor7LmSYqiQMQqEbA3JEYqhTXl7oiLiEBdQ/L3jWW1TYlW24QGQsDRL7PWsBR26iQgk3NQJEj6LMUOJ2BgEhGPeUUOHS0RQAIJ6xt0lgklbW+BIZR09UXK5nK0SAAJlez3RB24NFhJoE8iFoYixx/ULUS0SQD65tglJHAPYZyEDkORQ2EhdZhT3G2isJOZ13a4WpskoWdgLQP48A2776HsdTaw+Le0vyGpbjext4GTcBr3iQRD1FfEP4+ESLOfSJiWGeaJ3MbK+xgDTyGe5q27SJ56ciZBq/ZkXCfyAVJmuOn9DdJ8MiRwP++5WPoEmIxkS546QAAJVbR9ooJnEqASPDdfqO6ASUkbJXQ0ToBQgSI9Bc8jRi4NvSLSQP8LHW+WJm1s8eEfM1VT4SRMBb3OVHHzgNSX5DrzagOVw1BlT0W2KRJg4Hjo6wn3XKATccR2mUZrGXDzfNdtzOcj7lnyo+AgsGfc89NFWPZcZdL0pqW0ZAdmrgFURzAVCNhI0iiSoEJJwbh5CIcvSqA0AUOqPjbFPnm713gU2Ksik1QnLKn6sSk0N5IAIzkyJFA6SYQlT/lms2fW9BqYM2WSDl2yEQjPTZlqa7XKdYO3ieHr2OCtdWbb6r+x6VxdaHSrv0bEdR/60AZw3cd/2ICu9yAYG2CRI4FHSBA/EvjOqlpf50igjqs+HMpx1ceEOa76wLgEeBOl3938rwOI6B/Wps2Muu334wAAAABJRU5ErkJggg==\"","module.exports = \"data:image/png;base64,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\"","var map = {\n\t\"./c10_1.png\": 36,\n\t\"./c10_9.png\": 37,\n\t\"./c1_1.png\": 38,\n\t\"./c2_1.png\": 39,\n\t\"./c2_2.png\": 40,\n\t\"./c2_3.png\": 41,\n\t\"./c3_1.png\": 42,\n\t\"./c3_2.png\": 43,\n\t\"./c3_3.png\": 44,\n\t\"./c3_4.png\": 45,\n\t\"./c3_5.png\": 46,\n\t\"./c3_6.png\": 47,\n\t\"./c3_7.png\": 48,\n\t\"./c4_1.png\": 49,\n\t\"./c4_2.png\": 50,\n\t\"./c4_3.png\": 51,\n\t\"./c4_4.png\": 52,\n\t\"./c4_5.png\": 53,\n\t\"./c5_1.png\": 54,\n\t\"./c5_2.png\": 55,\n\t\"./c5_3.png\": 56,\n\t\"./c6_1.png\": 57,\n\t\"./c6_2.png\": 58,\n\t\"./c6_3.png\": 59,\n\t\"./c6_4.png\": 60,\n\t\"./c6_5.png\": 61,\n\t\"./c7_1.png\": 62,\n\t\"./c7_2.png\": 63,\n\t\"./c7_3.png\": 64,\n\t\"./c7_4.png\": 65,\n\t\"./c7_5.png\": 66,\n\t\"./c8_1.png\": 67,\n\t\"./c8_2.png\": 68,\n\t\"./c8_3.png\": 69,\n\t\"./c9_1.png\": 70,\n\t\"./c9_2.png\": 71,\n\t\"./c9_3.png\": 72,\n\t\"./c9_4.png\": 73,\n\t\"./c9_5.png\": 74,\n\t\"./c9_6.png\": 75,\n\t\"./c9_7.png\": 76\n};\n\n\nfunction webpackContext(req) {\n\tvar id = webpackContextResolve(req);\n\treturn __webpack_require__(id);\n}\nfunction webpackContextResolve(req) {\n\tvar id = map[req];\n\tif(!(id + 1)) { // check for number or string\n\t\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\t\te.code = 'MODULE_NOT_FOUND';\n\t\tthrow e;\n\t}\n\treturn id;\n}\nwebpackContext.keys = function webpackContextKeys() {\n\treturn Object.keys(map);\n};\nwebpackContext.resolve = webpackContextResolve;\nmodule.exports = webpackContext;\nwebpackContext.id = 35;","module.exports = __webpack_public_path__ + \"static/media/c10_1.d660ca3b.png\";","module.exports = __webpack_public_path__ + \"static/media/c10_9.719eeb48.png\";","module.exports = __webpack_public_path__ + \"static/media/c1_1.fc452a02.png\";","module.exports = __webpack_public_path__ + \"static/media/c2_1.19b414a6.png\";","module.exports = __webpack_public_path__ + \"static/media/c2_2.bbcd8128.png\";","module.exports = __webpack_public_path__ + \"static/media/c2_3.52ea6d65.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_1.47fc5477.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_2.ebff6bc8.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_3.1985ee06.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_4.18a0e779.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_5.2d3d4141.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_6.9a35e80b.png\";","module.exports = __webpack_public_path__ + \"static/media/c3_7.f864af43.png\";","module.exports = __webpack_public_path__ + \"static/media/c4_1.f3e17f54.png\";","module.exports = __webpack_public_path__ + \"static/media/c4_2.9d7979c4.png\";","module.exports = __webpack_public_path__ + \"static/media/c4_3.e9243049.png\";","module.exports = __webpack_public_path__ + \"static/media/c4_4.b38ee0ae.png\";","module.exports = __webpack_public_path__ + \"static/media/c4_5.63e7c944.png\";","module.exports = __webpack_public_path__ + \"static/media/c5_1.1b5c752a.png\";","module.exports = __webpack_public_path__ + \"static/media/c5_2.ceffa869.png\";","module.exports = __webpack_public_path__ + \"static/media/c5_3.610cee02.png\";","module.exports = __webpack_public_path__ + \"static/media/c6_1.27f07e1b.png\";","module.exports = __webpack_public_path__ + \"static/media/c6_2.2c3f0704.png\";","module.exports = __webpack_public_path__ + \"static/media/c6_3.5c47b06d.png\";","module.exports = __webpack_public_path__ + \"static/media/c6_4.7e0df0d3.png\";","module.exports = __webpack_public_path__ + \"static/media/c6_5.a7200b1e.png\";","module.exports = __webpack_public_path__ + \"static/media/c7_1.19847e06.png\";","module.exports = __webpack_public_path__ + \"static/media/c7_2.58ec847f.png\";","module.exports = __webpack_public_path__ + \"static/media/c7_3.6031e12c.png\";","module.exports = __webpack_public_path__ + \"static/media/c7_4.abaff87e.png\";","module.exports = __webpack_public_path__ + \"static/media/c7_5.cc3015ab.png\";","module.exports = __webpack_public_path__ + \"static/media/c8_1.c9cb9f7f.png\";","module.exports = __webpack_public_path__ + \"static/media/c8_2.24c36f44.png\";","module.exports = __webpack_public_path__ + \"static/media/c8_3.51fa02f1.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_1.6d4a1a3a.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_2.dc506648.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_3.7f7a433e.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_4.17d4e6c9.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_5.13bb8629.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_6.47587bdf.png\";","module.exports = __webpack_public_path__ + \"static/media/c9_7.5549c328.png\";","import React from \"react\"\nimport { Link } from \"react-router-dom\";\nimport check_gray_path from '../images/check_gray.png';\nimport check_green_path from '../images/check_green.png';\n\nexport default function ChapterButton(props) {\n    return (\n        \n        <div className=\"chapter-face\">\n            <Link className=\"chapter-face-container\" style={{ textDecoration: 'none', color: \"#3B3936\"}} to={`/chapter${props.value}/`}>\n                <img src={props.isClear ? check_green_path : check_gray_path} className=\"check_top-icon\" alt=\"correct icon\" />\n            \n                <p className=\"chapter-num\">{props.title}</p>\n            </Link>\n        </div>\n        \n    )\n}","import React from \"react\"\nimport '../css/Footer.css';\n\nexport default function Footer() {\n    return (\n        <div className=\"footer-container\">\n            \n            <p className=\"footer-txt\">© Copyright 2022 by 斎藤 康毅 (Koki Saitoh)</p>\n        </div>\n    )\n}","import React from \"react\"\nimport cover_path from '../images/cover.jpg';\nimport ChapterButton from \"./ChapterButton\";\nimport Footer from \"./Footer\";\nimport '../css/TopPage.css';\n\n\nexport default function TopPage(props) {\n    const chapterNum = 10\n    \n    const titleList = props.titleList.data\n\n    const [userLog, _] = React.useState(\n        () => {\n            const _userLog = {\n                clearCnt:0,\n                clearList:[]\n            }\n            let sumCorrect = 0\n    \n            for(let i=0; i <chapterNum; i++){\n                const _clearList = JSON.parse(localStorage.getItem(`book4-chapter${i+1}-clears`)) || [false]\n                \n                let _allCorrect = true\n                for(let k=0; k < _clearList.length; k++){\n                    if (_clearList[k]) {\n                        sumCorrect += 1\n                    }else{\n                        _allCorrect = false\n                    }\n                }\n\n                _userLog.clearList.push(_allCorrect)\n            }\n\n            _userLog.clearCnt = sumCorrect\n            return _userLog\n        }\n    )\n\n    const chapterButtonDom = []\n    for(let i=0; i <chapterNum; i++){\n        chapterButtonDom.push(\n        <ChapterButton key={i} isClear={userLog.clearList[i]} value={i+1} title={titleList[i]} clickHandler={props.clickHandler}/>\n        )\n    }\n\n    return (\n        <div className=\"main-container\">\n            <div className=\"top-container\">\n                <div className=\"top-txt-container\">\n                    <a href=\"https://www.amazon.co.jp/dp/4873119758/ref=cm_sw_r_tw_dp_7E324ZWVC18MSY0DRJ1G\" target=\"_blank\">\n                        <img src={cover_path} className=\"cover-img\" alt=\"book cover\" />\n                    </a>\n                    <h2 className=\"top-title\">強化学習100題</h2>\n                    <p className=\"top-subtitle\">『ゼロから作るDeep Learning ❹ ―強化学習編』</p>\n                    <p className=\"top-subtitle2\">確認テスト</p>\n                    \n                    <div className=\"top-clear-cnt-container\">\n                    <h2 style={{background: `linear-gradient(90deg,rgb(133, 253, 133) 0%,rgb(198, 254, 198) ${userLog.clearCnt}%, white ${userLog.clearCnt}%,white 100%)`}} className=\"top-clear-cnt\">\n                        <span className=\"small-result-txt\">正解数</span>\n                        <span className=\"result-top-num\">{userLog.clearCnt}</span>\n                        <span className=\"small-result-txt\">{`/ 100題`}</span>\n                        {/* {isClear && <span className=\"reward-btn\" onClick={reward}> 🎉</span> */}\n                    </h2>\n                    </div>\n\n\n                    \n                </div> \n                <div className=\"top-chapter-container\">\n                        {chapterButtonDom }\n                </div>\n\n                \n            </div>\n            <Footer />\n        </div>\n    )\n}","import React from \"react\"\nimport '../css/App.css';\nimport '../css/ProblemPage.css';\n\n\nexport default function SelectLable(props) {\n    const {lazyCheckList, checkList, answer, handleChange, text, id, value} = props\n\n\n    let cls = \"p-label\"\n    if(!lazyCheckList) {\n        cls += \"\"\n    }else if(lazyCheckList[id] === \"\"){\n        cls += \"\"\n    }else if(answer[id] === value) {\n        cls += \" p-correct\"\n    }else if((lazyCheckList[id] === answer[id]) && lazyCheckList[id] === value){\n        cls += \" p-correct\"\n    }else if(lazyCheckList[id] !== answer[id] && lazyCheckList[id] === value){\n        cls += \" p-wrong fadeout\"\n    }else {\n        cls += \" fadeout\"\n    }\n\n    return (\n        <div className={cls}>\n            <label className=\"radio-container\">\n                <input\n                type=\"radio\" \n                name={id}\n                className=\"radio-input\"\n                value={value}\n                checked={checkList[id] === value}\n                onChange={handleChange}\n                // required\n                disabled={lazyCheckList!==undefined}\n                    />\n                <div className=\"selection-raido-txt\" style={{display:\"inline\"}}dangerouslySetInnerHTML={{__html: text}} />\n            </label>\n        </div>\n    )\n}","\nconst problems = [\n    {\n        question: \"機械学習は問題の構造によって大きく3つに分けられる。その3つとは？\",\n        selection: [\n            \"強化学習 / 事前学習 / 教師なし学習\",\n            \"深層学習 / 強化学習 / 表現学習\",\n            \"教師あり学習 / 教師なし学習 / 強化学習\"\n        ],\n        img: null,\n        answer: \"3\",\n    },\n    {\n        question: \"強化学習は、<span>（　A　）</span>が<span>（　B　）</span>と相互作用しながら集めたデータを使って、高い<span>（　C　）</span>を得る方法を学習する問題設定である。\",\n        selection: [\n            \"<span>（A）ユーザー</span>　<span>（B）教師</span>　<span>（C）フィードバック</span>\",\n            \"<span>（A）エージェント</span>　<span>（B）環境</span>　<span>（C）報酬</span>\",\n            \"<span>（A）教師</span>　<span>（B）エージェント</span>　<span>（C）認識結果</span>\",\n        ],\n        img: null,\n        answer: \"2\",\n    },\n    {\n        question: \"次の図はバンディット問題の枠組みを表した図である。空欄に入る用語を答えなさい。\",\n        selection: [\n            \"<span>（A）行動<span/>　<span>（B）報酬<span/>　<span>（C）環境<span/>\",\n            \"<span>（A）報酬<span/>　<span>（B）価値<span/>　<span>（C）教師<span/>\",\n            \"<span>（A）行動<span/>　<span>（B）状態<span/>　<span>（C）環境<span/>\",\n        ],\n        img: \"c1_1.png\",\n        answer: \"1\",\n    },\n    {\n        question: \"バンディット問題では、報酬の期待値のことを<span>（　A　）</span>という特別な名前で呼ぶ。特に「行動に対して得られる報酬の期待値」は <span>（　B　）</span>と呼ばれる。\",\n        selection: [\n            \"（A）方策　　（B）行動収益\",\n            \"（A）収益　　（B）価値\",\n            \"（A）価値　　（B）行動価値\",\n        ],\n        img: null,\n        answer: \"3\",\n    },\n    {\n        question: \"バンディット問題で、<span class='math'>n</span> 回目の報酬を <span class='math'>R<span class='bottom'>n</span></span> として、その時点での行動価値（報酬の期待値）の推定値を <span class='math'>Q<span class='bottom'>n</span></span> とする。<span class='math'>Q<span class='bottom'>n</span></span>はどのように計算するのがふさわしいか？\",\n        selection: [\n            \"<span class='math'>R<span class='bottom'>1</span></span> + <span class='math'>R<span class='bottom'>2</span></span> + ... + <span class='math'>R<span class='bottom'>n</span></span>\",\n            \"<span class='math'>R<span class='bottom'>n</span>\",\n            \"<span class='frac'><span class='shi'><span class='math'>R<span class='bottom'>1</span></span> + <span class='math'>R<span class='bottom'>2</span></span> + ... + <span class='math'>R<span class='bottom'>n</span></span></span><span class='bo'><span class='math'>n</span></span></span>\",\n        ],\n        img: null,\n        answer: \"3\",\n    },\n    {\n        question: \"<br/><div class='center'><span class='math'>Q<span class='bottom'>n</span></span> = <span class='frac'><span class='shi'><span class='math'>R<span class='bottom'>1</span></span> + <span class='math'>R<span class='bottom'>2</span></span> + ... + <span class='math'>R<span class='bottom'>n</span></span></span><span class='bo'><span class='math'>n</span></span></span></div> <br> 上式を <span class='math'>Q<span class='bottom'>n-1</span></span> を使って “インクリメンタル” に計算するには？\",\n        selection: [\n            \"<span class='math'>Q<span class='bottom'>n</span></span>  = <span class='math'>Q<span class='bottom'>n-1</span></span> + <span class='frac'><span class='shi'><span class='math-num'>1</span></span><span class='bo'><span class='math'>n</span></span></span> <span class='math-num'>(<span class='math'>R</span><span class='bottom'>n</span> - <span class='math'>Q</span><span class='bottom'>n-1</span>)</span>\",\n            \"<span class='math'>Q<span class='bottom'>n</span></span>  = <span class='math'>Q<span class='bottom'>n-1</span></span> - <span class='frac'><span class='shi'><span class='math-num'>1</span></span><span class='bo'><span class='math'>n</span></span></span> <span class='math-num'><span class='math'>R</span><span class='bottom'>n</span></span>\",\n            \"<span class='math'>Q<span class='bottom'>n</span></span>  = <span class='frac'><span class='shi'><span class='math-num'>1</span></span><span class='bo'><span class='math'>n</span></span></span> <span class='math-num'>(<span class='math'>R</span><span class='bottom'>n</span> - <span class='math'>Q</span><span class='bottom'>n-1</span>)</span>\",\n        ],\n        img: null,\n        answer: \"1\",\n    },\n    {\n        question: \"スロットマシンを実際にプレイして得られた報酬は、ある<span>（　A　）</span>から生成された<span><span>（　B　）</span></span>である。そのため、実際に得られた報酬の平均値は<span>（　C　）</span>と呼ぶことができる。\",\n        selection: [\n            \"<span>（A）期待値</span>　　<span>（B）平均</span>　<span>（C）収益</span>\",\n            \"<span>（A）確率分布</span>　<span>（B）標本</span>　<span>（C）標本平均</span>\",\n            \"<span>（A）戦略</span>　<span>（B）サンプル</span>　<span>（C）行動価値</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n    {\n        question: \"「greedy」の説明として<b>誤っているもの</b>を1つを選べ。\",\n        selection: [\n            \"greedyとは日本語では「貪欲な」と訳す\",\n            \"greedyな手法とは、局所的に最善の手を選ぶことである\",\n            \"greedyな手法は常に最適解となる\",\n            \"greedyな行動は、別の表現として「活用」とも呼ばれる\"\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n    {\n        question: \"強化学習のアルゴリズムは、<span>（　A　）</span>のバランスをいかに取るかという点が重要である。ε-greedy 法は ε の確率で<span>（　B　）</span>を行い、それ以外は<span>（　C　）</span>を行う。\",\n        selection: [\n            \"<span>（A）活用と探索</span>　<span>（B）探索</span>　<span>（C）活用</span>\",\n            \"<span>（A）不確かさ</span>　<span>（B）行動</span>　<span>（C）探索</span>\",\n            \"<span>（A）収益</span>　<span>（B）活用</span>　<span>（C）探索</span>\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n    {\n        question: \"<span>（　A　）</span>の<span>（　B　）</span>が時間とともに変動する問題を非定常問題という。非定常問題を解くには、これまで得た報酬を平均するとき、新しい報酬ほど重みを<span>（　C　）</span>に大きくする必要がある。\",\n        selection: [\n            \"<span>（A）行動</span>　<span>（B）確率</span>　<span>（C）線形的</span>\",\n            \"<span>（A）報酬</span>　<span>（B）確率分布</span>　<span>（C）指数関数的</span>\",\n            \"<span>（A）収益</span>　<span>（B）確率密度</span>　<span>（C）均一</span>\",\n\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c1 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"マルコフ決定過程（MDP）について <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"MDPは、エージェントの行動によって環境の状態が変わる問題を扱う\",\n            \"MDPの「マルコフ（性）」とは、環境の状態遷移において、現在の状態だけにより次の状態の遷移が決まることを意味する\",\n            \"現実にあるすべての問題はMDPとして定式化できる\"\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の図はMDPの枠組みを表した図である。空欄に入る用語を答えなさい。\",\n        selection: [\n            \"<span>（A）状態 <span class='math'>S<span class='bottom'>t</span></span></span>　<span> （B）報酬 <span class='math'>R<span class='bottom'>t</span></span></span>　<span>（C）行動 <span class='math'>A<span class='bottom'>t</span></span></span>\",\n            \"<span>（A）状態 <span class='math'>S<span class='bottom'>t</span></span></span>　<span>（B）行動 <span class='math'>A<span class='bottom'>t</span></span></span>　<span>（C）報酬 <span class='math'>R<span class='bottom'>t</span></span></span>\",\n            \"<span>（A）報酬 <span class='math'>R<span class='bottom'>t</span></span></span> （B）行動 <span class='math'>A<span class='bottom'>t</span></span></span>　<span>（C）状態 <span class='math'>S<span class='bottom'>t</span></span></span>　<span>\",\n        ],\n        img: \"c2_1.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>MDPでは次の時刻に得られる報酬を最大化することを目標とする。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"状態遷移は、状態と<span>（　A　）</span>の2つを入力すると、次の状態が出力される。このとき、次の状態が一意に決まる場合は<span>（　B　）</span> <span class='math'>f(s, a)</span> によって、次の状態が<span>（　C　）</span>に決まる場合は確率 <span class='math'>p(s'</span> | <span class='math'>s, a)</span> によってモデル化できる。\",\n        selection: [\n            \"<span>（A）報酬</span>　<span>（B）確率</span>　<span>（C）決定論的</span>\",\n            \"<span>（A）行動</span>　<span>（B）関数</span>　<span>（C）確率的</span>\",\n            \"<span>（A）収益</span>　<span>（B）条件</span>　<span>（C）確率的</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"収益 <span class='math'>G<span class='bottom'>t</span></span> の定義式として正しいものはどれか。ただし割引率を <span class='math'>γ</span> とする。\",\n        selection: [\n            \"<span class='math'>G<span class='bottom'>t</span></span> = <span class='math'>R<span class='bottom'>t</span></span> + <span class='math' style='margin-right: .2em'>γ</span><span class='math'>R<span class='bottom'>t+1</span></span> + <span class='math'>γ</span><span class='up'>2</span><span class='math'>R<span class='bottom'>t+2</span></span> + ...\",\n\n            \"<span class='math'>G<span class='bottom'>t</span></span> = <span class='math'>R<span class='bottom'>t</span></span>\",\n\n            \"<span class='math'>G<span class='bottom'>t</span></span> = <span class='math' style='margin-right: .2em'>γ</span> ( <span class='math'>R<span class='bottom'>t</span></span> + <span class='math'>R<span class='bottom'>t+1</span></span> + <span class='math'>R<span class='bottom'>t+2</span></span> + ... )\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"方策について <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"エージェントの目標は最適方策を見つけることである\",\n            \"最適方策とは、次の時刻の報酬が最大となる方策である\",\n            \"マルコフ性により、エージェントの方策は現在の状態のみから行動を決めることができる\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"・ エージェントの行動の決め方を<span>（　A　）</span>という <br>・ MDPは問題によって<span>（　B　）</span>と連続タスクに分けられる<br>・ 環境は報酬関数と<span>（　C　）</span>によってモデル化できる\",\n        selection: [\n            \"<span>（A）方策</span>　<span>（B）エピソードタスク</span>　<span>（C）状態遷移確率</span>\",\n            \"<span>（A）行動関数</span>　<span>（B）一定タスク</span>　<span>（C）遷移関数</span>\",\n            \"<span>（A）行動確率</span>　<span>（B）確率タスク</span>　<span>（C）状態遷移関数</span>\",\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の状態価値関数の定義式の記述について <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"状態価値関数は、収益 <span class='math'>G<span class='bottom'>t</span></span> の期待値で表される\",\n            \"状態価値関数の条件は、状態 <span class='math'>S<span class='bottom'>t</span></span> が <span class='math'>s</span> であることと、エージェントの方策が <span class='math' style='font-style: normal'>π</span> であることの2つである\",\n            \"状態価値関数の出力はベクトルである\"\n        ],\n        img: \"c2_2.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"ここではMDPの問題設定として状態が5つ（<span class='math'>L</span><span class='math' style='font-style: normal'>1</span> <span class='math'>~</span> <span class='math'>L</span><span class='math' style='font-style: normal'>5</span>）あり、方策の候補として5つ（<span class='math' style='font-style: normal'>π</span><span class='bottom'>1</span>, <span class='math' style='font-style: normal'>π</span><span class='bottom'>2</span>, ... , <span class='math' style='font-style: normal'>π</span><span class='bottom' style='font-size: 0.85em;'>*</span>）ある場合を考える。このとき状態価値関数の図が次のグラフのようになったとき、図に関する記述として <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"最適方策は <span class='math' style='font-style: normal'>π</span><span class='bottom' style='font-size: 0.85em;'>*</span> である\",\n            \"方策 <span class='math' style='font-style: normal'>π</span><span class='bottom'>2</span> よりも方策 <span class='math' style='font-style: normal'>π</span><span class='bottom'>4</span> の方が優れている\",\n            \"状態 <span class='math'>L</span><span class='math' style='font-style: normal'>1</span> に限定すれば、方策 <span class='math' style='font-style: normal'>π</span><span class='bottom'>3</span> よりも方策 <span class='math' style='font-style: normal'>π</span><span class='bottom'>1</span> の方が優れている\"\n        ],\n        img: \"c2_3.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>どのようなMDPの問題であっても、決定論的な最適方策が少なくとも1つは存在する。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c2 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"収益 <span class='math'>G<span class='bottom'>t</span></span> は次の式で定義される。<br><div style='margin-top: 10px; margin-bottom: -10px; text-align: center'><span class='math'>G<span class='bottom'>t</span></span> = <span class='math'>R<span class='bottom'>t</span></span> + <span class='math' style='margin-right: .2em'>γ</span><span class='math'>R<span class='bottom'>t+1</span></span> + <span class='math'>γ</span><span class='up'>2</span><span class='math'>R<span class='bottom'>t+2</span></span> + ...</div><br>このとき、<span class='math'>G<span class='bottom'>t</span></span> と <span class='math'>G<span class='bottom'>t+1</span></span> の間で成り立つ式を1つ選べ。\",\n        selection: [\n            \"<span class='math'>G<span class='bottom'>t</span></span> = <span class='math'>R<span class='bottom'>t</span></span> + <span class='math' style='margin-right: .2em'>γ</span><span class='math'>G<span class='bottom'>t+1</span></span>\",\n\n            \"<span class='math'>G<span class='bottom'>t</span></span> = <span class='math' style='margin-right: .2em'>γ</span><span class='math'>R<span class='bottom'>t</span></span> + <span class='math'>G<span class='bottom'>t+1</span></span>\",\n\n            \"<span class='math'>G<span class='bottom'>t</span></span> =  <span class='math' style='margin-right: .2em'>γ</span><span class='math'>G<span class='bottom'>t+1</span></span>\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次のバックアップ線図について <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"状態が <span class='math'>s</span> のとき行動の候補は3つある\",\n            \"状態が <span class='math'>s</span> のとき行動 <span class='math'>a<span class='bottom'>2</span></span> を選択する確率は <span class='math' style='font-style: normal'>π</span>(<span class='math'>a = a<span class='bottom'>2</span></span> | <span class='math'>s</span>) である\",\n            \"状態が <span class='math'>s</span> のとき行動 <span class='math'>a<span class='bottom'>1</span></span> を選択して状態 <span class='math'>s<span class='bottom'>2</span></span> に遷移する確率は<br><span class='math_n'>π</span>(<span class='math'>a = a<span class='bottom'>1</span></span> | <span class='math'>s</span>)  +  <span class='math'>p</span>(<span class='math'>s' = s<span class='bottom'>2</span></span> | <span class='math'>s</span>, <span class='math'>a = a<span class='bottom'>1</span></span>) である\"\n        ],\n        img: \"c3_1.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"時刻 <span class='math'>t</span> における状態の確率変数を <span class='math'>S<span class='bottom'>t</span></span> 、報酬の確率変数を <span class='math'>R<span class='bottom'>t</span></span> 、報酬関数を <span class='math'>r <span style='font-style: normal'>(</span>s, a, s'<span style='font-style: normal'>)</span></span> とする。このとき次の式は正しいか？\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"c3_2.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次のベルマン方程式に関して <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"ベルマン方程式は「状態 <span class='math'>s</span> の状態価値関数」と「その次に取り得る状態 <span class='math'>s'</span> の状態価値関数」との関係性を表した式である\",\n            \"ベルマン方程式は状態遷移が確率的に決まる場合（決定論的でない場合）にのみ成り立つ\",\n            \"ベルマン方程式は、すべての状態 <span class='math'>s</span> とすべての方策 <span class='math_n'>π</span> において成り立つ\",\n        ],\n        img: \"c3_3.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"ベルマン方程式によって、<span>（　A　）</span>に続く計算を<span>（　B　）</span>に変換することができ、それを解くことで<span>（　C　）</span>を求めることができる。\",\n        selection: [\n            \"<span>（A）無限</span>　<span>（B）連立方程式</span>　<span>（C）状態価値関数</span>\",\n            \"<span>（A）確率的</span>　<span>（B）近似式</span>　<span>（C）収益</span>\",\n            \"<span>（A）決定論的</span>　<span>（B）連立方程式</span>　<span>（C）行動価値</span>\",\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の式は行動価値関数（Q関数）の定義式である。Q関数に関する記述で <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"Q関数は収益の期待値である\",\n            \"Q関数の条件は、方策が <span class='math_n'>π</span> であること、状態が <span class='math'>s</span>であること、そして行動が<span class='math'>a</span>であることの3つである\",\n            \"<span class='math'>q<span class='bottom'>π</span></span><span class='math_n'>(</span><span class='math'>s, a</span><span class='math_n'>)</span> の行動 <span class='math'>a</span> は方策 <span class='math_n'>π</span> によって決められる\"\n\n        ],\n        img: \"c3_4.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の式の<span>（　A　）</span>に入る数式は？\",\n        selection: [\n            \"<span class='math'>r <span style='font-style: normal'>(</span>s, a, s'<span style='font-style: normal'>)</span></span>\",\n            \"<span class='math' style='font-style: normal'>π</span>(<span class='math'>a</span> | <span class='math'>s</span>)\",\n            \"<span class='math'>p</span>(<span class='math'>s'</span> | <span class='math'>s</span>, <span class='math'>a</span>)\"\n\n        ],\n        img: \"c3_5.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式に関して <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"上式は、行動価値関数に関するベルマン最適方程式である\",\n            \"<span class='math'>q<span class='bottom'>*</span></span><span class='math_n'>(</span><span class='math'>s, a</span><span class='math_n'>)</span> は最適方策における行動価値関数であり、これは最適行動価値関数と呼ばれる\",\n            \"上式には max 演算子が使われているため、一般的には解くことができない\"\n        ],\n        img: \"c3_6.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>最適行動価値関数 <span class='math'>q<span class='bottom'>*</span></span><span class='math_n'>(</span><span class='math'>s, a</span><span class='math_n'>)</span> を得ることができれば、次式より最適方策 <span class='math'>μ<span class='bottom'>*</span></span><span class='math_n'>(</span><span class='math'>s</span><span class='math_n'>)</span> が求まる。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"c3_7.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>MDPでは決定論的な最適方策が少なくとも 1 つ存在する。このとき仮に最適方策が複数存在するとしたら、状態価値関数も複数存在することになる。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c3 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"ある方策 <span class='math' style='font-style: normal'>π</span> が与えられたときに、その方策の価値関数 <span class='math'>v<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>π</span></span></span><span class='math_n'>(</span><span class='math'>s</span><span class='math_n'>)</span> や <span class='math'>q<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>π</span></span></span><span class='math_n'>(</span><span class='math'>s, a</span><span class='math_n'>)</span> を求めることを<span>（　A　）</span>という。<br>方策を制御して<span>（　B　）</span>に近づけることを<span>（　C　）</span>という。\",\n        selection: [\n            \"<span>（A）方策評価</span>　<span>（B）最適方策</span>　<span>（C）方策制御</span>\",\n            \"<span>（A）方策制御</span>　<span>（B）最適方策</span>　<span>（C）方策評価</span>\",\n            \"<span>（A）方策評価</span>　<span>（B）最適行動</span>　<span>（C）調整</span>\",\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の式<span style='color:red'>(1)</span>と<span style='color:red'>(2)</span>について <b>誤っているもの</b> を一つ選べ。\",\n        selection: [\n            \"式<span style='color:red'>(1)</span>の <span class='math'>v</span> は真の価値関数であり、式<span style='color:red'>(2)</span>の <span class='math'>V</span> は推定値としての価値関数である\",\n\n            \"式<span style='color:red'>(2)</span>の <span class='math'>V<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>k+1</span></span></span><span class='math_n'>(</span><span class='math'>s</span><span class='math_n'>)</span> は <span class='math'>k </span><span class='math_n'> +1</span> 回目に更新された価値関数である\",\n            \n            \"式<span style='color:red'>(1)</span>はベルマン最適方程式である\",\n\n        ],\n        img: \"c4_1.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の式によって価値関数を更新するアルゴリズムを<span>（　A　）</span>という。このアルゴリズムでは、「推定値 <span class='math'>V<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>k</span></span></span><span class='math_n'>(</span><span class='math'>s'</span><span class='math_n'>)</span> 」を使って「別の推定値 <span class='math'>V<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>k+1</span></span></span><span class='math_n'>(</span><span class='math'>s</span><span class='math_n'>)</span> 」を改善していることから<span>（　B　）</span>と呼ばれる。\",\n        selection: [\n            \"<span>（A）動的計画法</span>　<span>（B）ボトムアップ</span>\",\n            \"<span>（A）方策制御</span>　<span>（B）トップダウン</span>\",\n            \"<span>（A）反復方策評価</span>　<span>（B）ブートストラップ</span>\",\n        ],\n        img: \"c4_2.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>動的計画法（ DP ）はアルゴリズムの総称である。それは、対象とする問題を小さな問題に分割して答えを求める手法一般を指す。反復方策評価も DP に属するアルゴリズムである。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"決定論的方策は各状態において行動が<span>（　A　）</span>に決まるので、<span>（　B　）</span>として表される。数式では<span>（　C　）</span>と表記する。\",\n        selection: [\n            \"<span>（A）複数</span>　<span>（B）分布</span>　<span>（C）<span class='math'><span class='math_n'>π(</span>s<span class='math_n'>|</span>a<span class='math_n'>)</span></span></span>\",\n            \"<span>（A）一意</span>　<span>（B）関数</span>　<span>（C）<span class='math'>μ<span class='math_n'>(</span>s<span class='math_n'>)</span></span></span>\",\n            \"<span>（A）確率的</span>　<span>（B）確率</span>　<span>（C）<span class='math'><span class='math_n'>π(</span>s<span class='math_n'>|</span>a<span class='math_n'>)</span></span></span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次式に関する記述で <b>誤っているもの</b> を1つ選べ。なお、ここでは現状の方策を <span class='math'>μ<span class='math_n'>(</span>s<span class='math_n'>)</span></span></span> として、更新された方策を <span class='math'>μ<span>'</span><span class='math_n'>(</span>s<span class='math_n'>)</span></span></span> とする。\",\n        selection: [\n            \"<span class='math_n'>argmax</span> は最大値を選び出す演算子である\",\n            \"上式による更新は「greedy化」と呼ぶことができる\",\n            \"上式の更新によって、方策 <span class='math'>μ<span>'</span><span class='math_n'>(</span>s<span class='math_n'>)</span></span></span> は常に改善される。もし改善されないのであれば、それが最適方策である\"\n        ],\n        img: \"c4_3.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"現状の方策をgreedy化することで方策が常に改善される数学的根拠は<span>（　A　）</span>によって与えられる。方策の評価と改善（greedy化）を交互に繰り返すアルゴリズムを<span>（　B　）</span>と呼ぶ。\",\n        selection: [\n            \"<span>（A）ベルマン方程式</span>　<span>（B）方策勾配法</span>\",\n            \"<span>（A）方策改善定理</span>　<span>（B）方策反復法</span>\",\n            \"<span>（A）動的計画法</span>　<span>（B）Q学習</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の図は方策反復法による価値関数と方策の推移のイメージ図である。この図について <b>誤っているもの</b> を一つ選べ\",\n        selection: [\n            \"「評価」と「改善」を交互に繰り返すことで最適方策に近づく\",\n            \"（理論的には）無限回繰り返すことで最適方策に収束する\",\n            \"方策と価値関数がとり得る空間は図のような2次元上になる\"\n        ],\n        img: \"c4_4.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"「評価」と「改善」の“粒度”を最小限に行うことで、ひとつの更新式によって最適方策へと近づくアルゴリズムを何というか？\",\n        selection: [\n            \"価値反復法\",\n            \"一般方策反復\",\n            \"プランニング\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の式で表される価値反復法について <b>誤っているもの</b> を1つ選べ。\",\n        selection: [\n            \"価値反復法はベルマン最適方程式を更新式に変更したものである\",\n            \"価値反復法は、任意の方策を使って価値関数を評価する\",\n            \"価値反復法は max 演算子を使って価値関数を直接更新する\"\n        ],\n        img: \"c4_5.png\",\n        answer: \"2\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c4 = p","\nconst problems = [\n    \n    {\n        question: \"次の記述は正しいか？<br><br>動的計画法（DP）を使って最適方策を得るには、環境のモデルが既知である必要がある。環境のモデルとは、状態遷移確率と報酬関数のことである。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"モンテカルロ法とは、データの<span>（　A　）</span>を繰り返し行って、その結果から推定する手法の総称である。強化学習では、モンテカルロ法を使うことで、経験から<span>（　B　）</span>を推定することができる。\",\n        selection: [\n            \"<span>（A）更新</span>　<span>（B）方策</span>\",\n            \"<span>（A）変形</span>　<span>（B）ベルマン方程式</span>\",\n            \"<span>（A）サンプリング</span>　<span>（B）価値関数</span>\",\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"サイコロを振るような確率的な試行を確率分布として表すモデルは<span>（　A　）</span>という。一方、サンプリングさえできれば良いというモデルは<span>（　B　）</span>という。\",\n        selection: [\n            \"<span>（A）サンプルモデル</span>　<span>（B）分布モデル</span>\",\n            \"<span>（A）分布モデル</span>　<span>（B）サンプルモデル</span>\",\n            \"<span>（A）確率モデル</span>　<span>（B）遷移モデル</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式<span style='color:red'>(1)</span>は平均値を求める計算式である。式<span style='color:red'>(2)</span>は“インクリメンタルな方式”で平均値を求める計算式である。式<span style='color:red'>(2)</span>の(A)(B)に入る数式を選べ。\",\n        selection: [\n            \"<span>（A）<span class='math'>V<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>n-1</span></span></span>　<span>（B）<span class='math'>s<span class='bottom'>n</span></span></span>\",\n            \"<span>（A）<span class='math'>V<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>n</span></span></span>　<span>（B）<span class='math'>s<span class='bottom'>n</span></span></span>\",\n            \"<span>（A）<span class='math'>V<span class='bottom'><span style='font-size: 0.9em; margin-right: 2px'>n+1</span></span></span>　<span>（B）<span class='math'>s<span class='bottom'>n-1</span></span></span>\",\n        ],\n        img: \"c5_1.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>モンテカルロ法による方策評価は、連続タスクでしか行えない。なぜなら連続タスクには「終わり」がないので、収益のサンプルデータを無限に更新できるからである。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の図は、状態 <span class='math'>A</span> からスタートして、方策 <span class='math_n'>π</span> に従って行動した結果である。図のとおり、<span class='math'>A</span>, <span class='math'>B</span>, <span class='math'>C</span> という順に状態を経てゴールにたどり着き、その間に得られる報酬は <span class='math'>R</span><span class='bottom'>0</span>, <span class='math'>R</span><span class='bottom'>1</span>, <span class='math'>R</span><span class='bottom'>2</span> とする。このタスクでの割引率を <span class='math'>γ</span> としたとき、状態 <span class='math'>A</span> からスタートして得られる収益 <span class='math'>G</span><span class='bottom'>A</span> はどのように表されるか？\",\n        selection: [\n            \"<span class='math'>R</span><span class='bottom'>0</span> + <span class='math'>R</span><span class='bottom'>1</span> + <span class='math'>R</span><span class='bottom'>2</span>\",\n            \"<span class='math'>R</span><span class='bottom'>1</span> + <span class='math'>γ</span> <span class='math'>R</span><span class='bottom'>2</span>\",\n            \"<span class='math'>R</span><span class='bottom'>0</span> + <span class='math'>γ</span> <span class='math'>R</span><span class='bottom'>1</span> + <span class='math'>γ</span><span class='up'>2</span><span class='math'>R</span><span class='bottom'>2</span>\",\n        ],\n        img: \"c5_2.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"「問題6」の図において、状態 <span class='math'>B</span> からスタートして得られる収益 <span class='math'>G</span><span class='bottom'>B</span> はどのように表されるか？\",\n        selection: [\n            \"<span class='math'>R</span><span class='bottom'>0</span> + <span class='math'>R</span><span class='bottom'>1</span> + <span class='math'>R</span><span class='bottom'>2</span>\",\n            \"<span class='math'>R</span><span class='bottom'>1</span> + <span class='math'>γ</span> <span class='math'>R</span><span class='bottom'>2</span>\",\n            \"<span class='math'>R</span><span class='bottom'>0</span> + <span class='math'>γ</span> <span class='math'>R</span><span class='bottom'>1</span> + <span class='math'>γ</span><span class='up'>2</span><span class='math'>R</span><span class='bottom'>2</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"モンテカルロ法を使って最適方策を求める方法に関して、次の記述のうち<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"環境のモデルにアクセスできない場合があるので、状態価値関数ではなく状態行動価値関数（Q関数）を対象にして評価と改善を行う\",\n            \"改善のフェーズでは「探索」が必要なので、ε-greedy法を使う\",\n            \"エピソードタスクと連続タスクのどちらのタスクであっても、モンテカルロ法を使って最適方策を求めることができる\"\n\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の式は、評価フェーズにおけるQ関数の更新式である。 <span class='math'>n</span> 回目の収益 <span class='math'>G</span><span class='up'>(n)</span>を得た時点でQ関数に関して、各収益の割合が図のように指数関数的に増加するには、式の(A)には何を入れるべきか？\",\n        selection: [\n            \"<span class='math'>α</span> （固定値）\",\n            \"<span class='math'>n</span>\",\n            \"<span class='frac'><span class='shi'><span class='math-num'>1</span></span><span class='bo'><span class='math'>n</span></span></span>\"\n        ],\n        img: \"c5_3.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"・自分とは別の場所で得られた経験から自分の方策を改善するアプローチを<span>（　A　）</span>と言う。<br>・エージェントの方策は、役割的に見ると、ターゲット方策と<span>（　B　）</span>の2つがある。<br>・2つの異なる方策を使って価値関数を評価するには<span>（　C　）</span>を使う\",\n        selection: [\n            \"<span>（A）方策オフ型</span>　<span>（B）挙動方策</span>　<span>（C）重点サンプリング</span>\",\n            \"<span>（A）方策オン型</span>　<span>（B）Q関数</span>　<span>（C）方策反復法</span>\",\n            \"<span>（A）プランニング</span>　<span>（B）最適方策</span>　<span>（C）方策勾配法</span>\",\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c5 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"次の記述は正しいか？<br><br>TD法はエピソードタスクと連続タスクの両方で使うことができる。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次式はTD法による状態価値関数の更新式である。(A)には何が入るか？\",\n        selection: [\n            \"<span class='math'>G<span class='bottom'>t</span>\",\n            \"<span class='math'>R</span><span class='bottom'>t</span> + <span class='math'>γ</span><span class='math'>V</span><span class='bottom'>π</span><span class='math_n'>(</span><span class='math'>S<span class='bottom'>t+1</span><span class='math_n'>)</span>\",\n            \"<span class='math'>R<span class='bottom'>t</span>\"\n        ],\n        img: \"c6_1.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"TD法を使った方策制御には、代表的なアルゴリズムが2つある。1つは<span>（　A　）</span>、もう1つは<span>（　B　）</span>である。\",\n        selection: [\n            \"<span>（A）SARSA</span>　<span>（B）Q学習</span>\",\n            \"<span>（A）重点サンプリング</span>　<span>（B）勾配法</span>\",\n            \"<span>（A）方策反復法</span>　<span>（B）模倣学習</span>\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"SARSAに関して<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"評価と改善のプロセスを交互に行う\",\n            \"評価フェーズではTD法により行動価値関数（Q関数）を更新する\",\n            \"SARSAは（一般的には）方策オン型であるため、改善フェーズでは方策を完全にgreedy化できる\"\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    \n\n    {\n        question: \"SARSAは方策オフ型に拡張することもできる。その点に関して<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"挙動方策とターゲット方策はできるだけ異なる確率分布である方が結果は安定する\",\n            \"2つの方策（挙動方策とターゲット方策）が異なるため、重点サンプリングを使って補正する必要がある\",\n            \"現状のQ関数に対して挙動方策は ε-greedy に更新し、 ターゲット方策は greedy に更新するのが一般的である\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"Q学習は<span>（　A　）</span>の手法であり、<span>（　B　）</span>にもとづき価値関数を更新する。このとき、<span>（　C　）</span>を必要としない。\",\n        selection: [\n            \"<span>（A）方策オン型</span>　<span>（B）モンテカルロ法</span>　<span>（C）探索</span>\",\n            \"<span>（A）方策オフ型</span>　<span>（B）TD法</span>　<span>（C）重点サンプリング</span>\",\n            \"<span>（A）方策イン型</span>　<span>（B）動的計画法</span>　<span>（C）サンプリング</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式とバックアップ線図は何を表しているか？\",\n        selection: [\n            \"ベルマン方程式\",\n            \"ベルマン最適方程式\",\n            \"SARSAによる価値関数の更新式\"\n        ],\n        img: \"c6_2.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次図のように、ベルマン方程式を“サンプリング版”に変更することで得られるアルゴリズムは何か？\",\n        selection: [\n            \"TD法\",\n            \"Q学習\",\n            \"SARSA\"\n        ],\n        img: \"c6_3.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の式とバックアップ線図は何を表しているか？\",\n        selection: [\n            \"ベルマン方程式\",\n            \"ベルマン最適方程式\",\n            \"Q学習による価値関数の更新式\"\n        ],\n        img: \"c6_4.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式と図のように、ベルマン最適方程式を“サンプリング版”に変更することで得られるアルゴリズムは何か？\",\n        selection: [\n            \"TD法\",\n            \"Q学習\",\n            \"SARSA\"\n        ],\n        img: \"c6_5.png\",\n        answer: \"2\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c6 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"次の(A)(B)(C)に入る用語を選べ。\",\n        selection: [\n            \"<span>（A）行列</span>　<span>（B）配列</span>　<span>（C）テンソル</span>\",\n            \"<span>（A）テンソル</span>　<span>（B）ベクトル</span>　<span>（C）行列</span>\",\n            \"<span>（A）スカラ</span>　<span>（B）ベクトル</span>　<span>（C）行列</span>\",\n        ],\n        img: \"c7_1.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次は行列の積の計算である。(A)に入る数値を選べ。\",\n        selection: [\n            \"<span class='math_n'>19</span>\",\n            \"<span class='math_n'>15</span>\",\n            \"<span class='math_n'>17</span>\"\n        ],\n        img: \"c7_2.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"・関数の最小値(もしくは最大値)を取る「関数の引数」を見つける作業を<span>（　A　）</span>という。<br>・<span>（　B　）</span>は関数の出力を最も大きくする方向を示します。<br>・勾配の方向にある距離だけ進み、その進んだ場所で再度勾配を求めるという作業を繰り返しながら徐々に最小値に近づく手法を<span>（　C　）</span>という。\",\n        selection: [\n            \"<span>（A）最大化</span>　<span>（B）確率</span>　<span>（C）誤差逆伝播法</span>\",\n            \"<span>（A）最適化</span>　<span>（B）勾配</span>　<span>（C）勾配降下法</span>\",\n            \"<span>（A）最小化</span>　<span>（B）微分</span>　<span>（C）反復法</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"実数値である <span class='math'>y</span> を、<span class='math'>x</span> の値から予測することは<span>（　A　）</span>と呼ばれる。さらに、その予測するモデルを「線形として（直線として）」予測するとき、それは<span>（　B　）</span>と呼ばれる。\",\n        selection: [\n            \"<span>（A）回帰</span>　<span>（B）線形回帰</span>\",\n            \"<span>（A）分類</span>　<span>（B）線形分類</span>\",\n            \"<span>（A）回帰</span>　<span>（B）確率回帰</span>\",\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"ニューラルネットワークの “悪さ” を評価する関数を総じて何というか。\",\n        selection: [\n            \"損失関数\",\n            \"目的関数\",\n            \"活性化関数\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次のニューラルネットワークに関する記述において<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"ニューラルネットワークは、線形変換と非線形変換を交互に行うように構成する\",\n            \"線形変換で行う計算は、行列の積によって行える\",\n            \"非線形変換で行う計算は、重みとバイアスを使用する\"\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次のグラフに関して<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"左のグラフはシグモイド関数、右のグラフはReLU関数である\",\n            \"左のグラフは非線形変換、右のグラフは線形変換を表す\",\n            \"2つの関数ともにニューラルネットワークの活性化関数として使うことができる\",\n        ],\n        img: \"c7_3.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次のディープラーニングのフレームワークに関する記述において<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"よく使われる処理が「レイヤ」や「関数」として用意されている\",\n            \"オプティマイザの役割はニューラルネットワークの順伝播を行うことである\",\n            \"バックプロパゲーションの仕組みがあり、自動で勾配（微分）が求めることができる\"\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"Q学習をニューラルネットワークを使って行う場合、次の図のどちらのネットワーク構造が効率的か？\",\n        selection: [\n            \"<span style='color:red'>(A)</span>\",\n            \"<span style='color:red'>(B)</span>\"\n\n        ],\n        img: \"c7_4.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式はQ学習の更新式である。ニューラルネットワークをこの式に適用するには、入力が<span>（　A　）</span>のとき出力が<span>（　B　）</span>となるようにニューラルネットワークを学習すれば良い\",\n        selection: [\n            \"<span>（A）<span class='math'>S</span><span class='bottom'>t</span> , <span class='math'>A</span><span class='bottom'>t</span></span>　<span>（B）<span class='math'>R</span><span class='bottom'>t</span></span>\",\n\n            \"<span>（A）<span class='math'>S</span><span class='bottom'>t</span>　<span>（B）<span class='math'>γ</span> <span class='math_n'>max</span><span class='bottom_i'>a</span> <span class='math'>Q</span><span class='math_n'>(</span><span class='math'>S<span class='bottom'>t+1</span>, <span class='math'>a</span></span> <span class='math_n'>)</span> </span>\",\n\n            \"<span>（A）<span class='math'>S</span><span class='bottom'>t</span> , <span class='math'>A</span><span class='bottom'>t</span></span>　<span>（B）<span class='math'>R</span><span class='bottom'>t</span> + <span class='math'>γ</span> <span class='math_n'>max</span><span class='bottom_i'>a</span> <span class='math'>Q</span><span class='math_n'>(</span><span class='math'>S<span class='bottom'>t+1</span>, <span class='math'>a</span></span> <span class='math_n'>)</span> </span>\",\n        ],\n        img: \"c7_5.png\",\n        answer: \"3\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c7 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"Q学習では推定値を使って推定値を更新するが、この原理（推定値を使って推定値を更新する原理）を何というか？\",\n        selection: [\n            \"ブートストラッピング\",\n            \"プランニング\",\n            \"サンプリング\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"「状態」と「観測」という用語に関する次の記述で<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"状態とは環境に関しての「完全な記述(情報)」であり、状態がわかればマルコフ決定過程により次の状態と報酬の確率分布が完全に決定する\",\n            \"観測とは状態の一部であり、環境に関しての「部分的な記述」である\",\n            \"どのようなタスクであっても状態と観測がイコールになることはない\"\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"DQNは強化学習のアルゴリズムである<span>（　A　）</span>をベースにして、そこにニューラルネットワークを用いた手法である。その特徴は、学習を安定させるために<span>（　B　）</span>と<span>（　C　）</span>という技術を使っている。\",\n        selection: [\n            \"<span>（A）TD法</span>　<span>（B）正規化</span>　<span>（C）ミニバッチ</span>\",\n            \"<span>（A）Q学習<span>（B）経験再生</span>　<span>（C）ターゲットネットワーク</span>\",\n            \"<span>（A）SARSA</span>　<span>（B）前処理</span>　<span>（C）報酬クリッピング</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"経験再生に関する次の記述で<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"エージェントが経験したデータを一度「バッファ」に保存する\",\n            \"Q関数を更新する際には、バッファから経験データをランダムに取り出して使う\",\n            \"経験再生は、方策オフ型と方策オン型の両方のアルゴリズムで使うことができる\"\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>経験再生によって経験データ間の相関が弱まり、偏りの少ないデータが得られる。さらに、経験データを繰り返し使うことができるため、データ効率が良くなる。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"ターゲットネットワークに関する次の記述で<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"Q関数を表すオリジナルのニューラルネットワークとは別にもう1つ同じ構造のニューラルネットワークを用意する\",\n            \"2つのニューラルネットワークは重みを一度だけ同期する\",\n            \"TDターゲットの値を計算するために用いるニューラルネットワークは、SGDなどの最適化手法によって重みを更新しない\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"2013年に論文で発表されたDQNでは<span>（　A　）</span>というゲーム環境での研究例が示されている。ニューラルネットワークへの入力は画像でり、入力画像に対しては<span>（　B　）</span>が行われ、<span>（　C　）</span>によるニューラルネットワークよって処理される。\",\n        selection: [\n            \"<span>（A）GAME BOY</span>　<span>（B）正規化</span>　<span>（C）RNN</span>\",\n            \"<span>（A）Atari</span>　<span>（B）前処理</span>　<span>（C）CNN</span>\",\n            \"<span>（A）セガ・マークⅢ</span>　<span>（B）データ拡張</span>　<span>（C）Transformer</span>\",\n            \n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次のグラフに関して<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"上のグラフはε-greedy法のεの調整方法を示したグラフである\",\n            \"グラフのようにεを調整することで、エージェントの経験が増えるに従って探索の割合を減らすことができる\",\n            \"εの調整方法は様々な方法が考えられるが、上のグラフのように設定するのがベストである\",\n        ],\n        img: \"c8_1.png\",\n        answer: \"3\",\n    },\n\n    {\n        question: \"次の式で表される関数 <span class='math'>A</span><span class='bottom'>π</span><span class='math_n'>(</span><span class='math'>s, a</span><span class='math_n'>)</span> は何というか？\",\n        selection: [\n            \"アドバンテージ関数\",\n            \"行動価値関数\",\n            \"アクション関数\"\n        ],\n        img: \"c8_2.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"Dueling DQNは次の図のようにニューラルネットワークをモデル化する。2つの出力があり、一つが<span>（　A　）</span>を表し、もうひとつがアドバンテージ関数を表す。そして、その2つを合わせることで<span>（　B　）</span>を出力するように構成したネットワーク構造である。\",\n        selection: [\n            \"<span>（A）最適状態価値関数</span>　<span>（B）指数関数</span>\",\n            \"<span>（A）Q関数</span>　<span>（B）アドバンテージ関数</span>\",\n            \"<span>（A）状態価値関数</span>　<span>（B）Q関数</span>\",\n        ],\n        img: \"c8_3.png\",\n        answer: \"3\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c8 = p\n","\nconst problems = [\n    \n    {\n        question: \"Q学習や SARSA、モンテカルロ法などの価値関数をモデル化する手法は、<span>（　A　）</span>と呼ばれる。一方、価値関数を経由せずに方策を直接モデル化する手法は<span>（　B　）</span>である。\",\n        selection: [\n            \"<span>（A）行動ベースの手法</span>　<span>（B）報酬ベースの手法</span>\",\n            \"<span>（A）価値ベースの手法</span>　<span>（B）方策ベースの手法</span>\",\n            \"<span>（A）収益ベースの手法</span>　<span>（B）価値ベースの手法</span>\",\n        ],\n        img: \"\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"方策をニューラルネットワークなどでモデル化し、勾配を使って方策を最適化する手法をなんというか？\",\n        selection: [\n            \"方策勾配法\",\n            \"方策反復法\",\n            \"TD法\",\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の数式は「最も単純な方策勾配法」の導出過程の一部である。次の数式に関して<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"<span class='math'>τ</span> は「状態、行動、報酬」からなる時系列データであり、「軌道(Trajectory)」とも呼ばれる\",\n            \"<span class='math'>G<span class='math_n'>(</span>τ<span class='math_n'>)</span></span> は収益であり、ここではエピソードタスクを考えているので無限に続く計算となる\",\n            \"<span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> は収益の期待値として表され、これを目的関数として最適化するのが「最も単純な方策勾配法」である\"\n\n        ],\n        img: \"c9_1.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の数式は「最も単純な方策勾配法」の導出過程の一部である。数式中の <span class='math_n'>∇</span><span class='bottom' style='margin-right:3px; font-style:italic'>θ</span><span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> は何を表すか？\",\n        selection: [\n            \"<span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> の <span class='math'>θ</span> に関する勾配\",\n            \"<span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> の期待値\",\n            \"<span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> の <span class='math'>θ</span> に関する発散\"\n        ],\n        img: \"c9_2.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次式は目的関数 <span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> をモンテカルロ法によって近似する方法を表す。これに関して<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"方策 <span class='math'>π</span><span class='bottom_i'>θ</span> のエージェントに実際に行動させ、軌道 <span class='math'>τ</span> を <span class='math'>n</span> 個得る\",\n            \"<span class='math'>x</span><span class='up'>(<span clss='math'>i</span>)</span> は期待値であるので実際には計算できない\",\n            \"<span class='math'>x</span><span class='up'>(<span clss='math'>i</span>)</span> の  <span class='math'>n</span> 個の平均を求めることで <span class='math_n'>∇</span><span class='bottom' style='margin-right:3px; font-style:italic'>θ</span><span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> を近似する\"\n\n        ],\n        img: \"c9_3.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式は<span>（　A　）</span>というアルゴリズムで使われる数式である。重要な点は <span class='math'>G<span class='bottom_i'>t</span></span> のように、時刻 <span class='math'>t</span> 以降の<span>（　B　）</span>を使うことである。\",\n        selection: [\n            \"<span>（A）REINFORCE</span>　<span>（B）収益</span>\",\n            \"<span>（A）Actor-Critic</span>　<span>（B）報酬</span>\",\n            \"<span>（A）Policy Gradient</span>　<span>（B）行動</span>\",\n        ],\n        img: \"c9_4.png\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次式のように <span class='math'>G<span class='bottom_i'>t</span></span> の代わりに <span class='math'>G<span class='bottom_i'>t</span></span> − <span class='math'>b<span class='bottom_i' style='margin-right:3px'>t</span></span><span class='math_n'>(</span><span class='math'>S<span class='bottom_i'>t</span></span><span class='math_n'>)</span> を使うことができる。適切な <span class='math'>b<span class='bottom_i' style='margin-right:3px'>t</span></span><span class='math_n'>(</span><span class='math'>S<span class='bottom_i'>t</span></span><span class='math_n'>)</span> を使うことで、<span class='math_n'>∇</span><span class='bottom' style='margin-right:3px; font-style:italic'>θ</span><span class='math'>J<span class='math_n'>(</span>θ<span class='math_n'>)</span></span> の分散を減らすテクニックを何というか？\",\n        selection: [\n            \"量子化\",\n            \"ベースライン\",\n            \"ミニバッチ\",\n        ],\n        img: \"c9_5.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の式の方策 <span class='math_n'>π</span><span class='bottom_i'>θ</span> と価値関数 <span class='math'>V</span><span class='bottom_i'>w</span> をニューラルネットワークでモデル化し、その2つのニューラルネットワークを並行して学習するアルゴリズムを何というか？\",\n        selection: [\n            \"DQN\",\n            \"Actor-Critic\",\n            \"REINFORCE\",\n        ],\n        img: \"c9_6.png\",\n        answer: \"2\",\n    },\n\n    {\n        question: \"次の記述は正しいか？<br><br>Actor-CriticのActorとは「行為者」という意味であり、方策 <span class='math_n'>π</span><span class='bottom_i'>θ</span> に該当する。一方、Criticは「批評家」という意味であり、価値関数 <span class='math'>V</span><span class='bottom_i'>w</span> に該当する。\",\n        selection: [\n            \"Yes（正しい）\",\n            \"No（誤り）\"\n        ],\n        img: \"\",\n        answer: \"1\",\n    },\n\n    {\n        question: \"次の記述は方策ベースの手法の利点を書いたものである。この中で<b>誤っているもの</b>を1つ選べ。\",\n        selection: [\n            \"方策を直接モデル化するので効率的である\",\n            \"連続的な行動空間でも使える\",\n            \"行動の選択確率がスムーズに変化しない\"\n\n        ],\n        img: \"\",\n        answer: \"3\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c9 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","\nconst problems = [\n    \n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"<div class='center'><b>準備中です... </b><br><a href='https://docs.google.com/forms/d/e/1FAIpQLSdsuFU_3h1nUlheZU8fF1kFNQRXMK6EMrPuOQicIW8lV7GuSg/viewform' target='_blank'>問題案を募集しています</a></div>\",\n        selection: [\n\n        ],\n        img: \"\",\n        answer: \"9\",\n    },\n\n    {\n        question: \"『ゼロから作るDeep Learning ❹ ―強化学習編 』の帯のコピーは？\",\n        selection: [\n            \"作る経験はコピーできない。\",\n            \"変わるもの、変わらないもの。\",\n            \"作るからこそ見えるもの。\"\n        ],\n        img: \"c10_9.png\",\n        answer: \"2\",\n    },\n\n]\n\nconst p = {}\nfor(let i=0; i < problems.length; i++)p[`p${i+1}`] = problems[i]\nexport const c10 = p\n\n\n// <span class='frac'><span class='shi'><span class='math'>分子</span></span><span class='bo'><span class='math'>分母</span></span></span>","import React from \"react\"\nimport SelectLable from \"./SelectLabel\";\nimport check_wrong_path from '../images/check_wrong.png';\nimport check_green_path from '../images/check_green.png';\nimport check_green_light_path from '../images/check_green2.png';\nimport check_gray_path from '../images/check_gray.png';\nimport back_on_path from '../images/back_on.png';\n\nimport { Link } from \"react-router-dom\";\nimport { useReward } from 'react-rewards';\n\nimport { c1 } from \"./problems/c1\";\nimport { c2 } from \"./problems/c2\";\nimport { c3 } from \"./problems/c3\";\nimport { c4 } from \"./problems/c4\";\nimport { c5 } from \"./problems/c5\";\nimport { c6 } from \"./problems/c6\";\nimport { c7 } from \"./problems/c7\";\nimport { c8 } from \"./problems/c8\";\nimport { c9 } from \"./problems/c9\";\nimport { c10 } from \"./problems/c10\";\n\nimport '../css/App.css';\nimport '../css/ProblemPage.css';\nimport Footer from \"./Footer\";\n\nconst all_problems = [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10]\n\nexport default function ProblemPage(props) {\n    \n    const chapter = props.chapter\n\n    const p_base = all_problems[chapter-1]\n    const p_length = Object.keys(p_base).length\n    const answer = {}\n    const checkListInit = {}\n\n    const chapterNum = 10\n    const title = props.title\n    const isLastChapter = chapter === chapterNum\n\n    const [problemClearList, setProblemClearList] = React.useState(()=>{\n        const initClearList = new Array();\n        for(let i=0 ;i <p_length; i++){\n            initClearList.push(false)\n        }\n        return JSON.parse(localStorage.getItem(`book4-chapter${chapter}-clears`)) || initClearList})\n\n    let isAllClear = true\n    for(let i=0; i <problemClearList.length; i++){\n        if(!problemClearList[i]){\n            isAllClear = false\n            break\n        }\n    }\n\n    const [isClear, setIsClear] = React.useState(false)\n\n    const [isEnd, setIsEnd] = React.useState(false)\n    const [correctCnt, setCorrectCnt] = React.useState(0)\n    const [checkList, setCheckList] = React.useState(checkListInit)\n    const [lazyCheckList, setLazyCheckList] = React.useState(undefined)\n\n    const { reward, isAnimating } = useReward('rewardId', 'confetti', {\n        // lifetime: 300,\n        position: \"absolute\"\n    });\n\n    function checkClear(){\n        let allCorrect = true\n        let cnt = 0\n        const yourCheckList = new Array()\n\n        for(let i=0; i<p_length; i++){\n            const key = `p${i+1}`\n\n            let isCorrect = false\n            if(checkList[key] === answer[key]){\n                cnt += 1\n                isCorrect = true\n            }else{\n                allCorrect = false\n            }\n\n            // 過去に正解した場合はlocalstorageは正解のままに\n            if(problemClearList[i]){\n                isCorrect = true\n            }\n            yourCheckList.push(isCorrect)\n        }\n\n        setProblemClearList(yourCheckList)\n        localStorage.setItem(`book4-chapter${chapter}-clears`, JSON.stringify(yourCheckList))\n\n        setCorrectCnt(cnt)\n        if(allCorrect) {\n            setIsClear(true)\n        }\n\n    }\n\n    function onSubmit(event) {\n        event.preventDefault();\n        setLazyCheckList(checkList)\n        setIsEnd(true)\n        checkClear()\n      }\n\n    function handleChange(event) {\n        const {name, value} = event.target\n        setCheckList(oldData => {\n            return {...oldData,\n                [name]: value\n            }\n        })\n    }\n\n    function reset(){\n        setIsEnd(false)\n        setLazyCheckList(undefined)\n        setCheckList(checkListInit)\n    }\n\n    React.useEffect(() => {\n        if (isClear) {\n            // reward()\n            setTimeout(reward, 500)\n            setTimeout(reward, 1500)\n        }\n    }, [isClear]);\n\n\n    function SeleclLabels(){ \n        const problemDomList = []\n\n        for(let i=1; i < p_length+1; i ++){\n            const p_key = `p${i}`\n            const q = p_base[p_key]\n\n            const selection_list = q.selection\n            answer[p_key] = q.answer\n            checkListInit[p_key] = \"\"\n\n            const selectionDom = selection_list.map((value, index)=>{\n                return <SelectLable \n                    checkList={checkList}\n                    lazyCheckList={lazyCheckList}\n                    answer={answer}\n                    handleChange={handleChange}\n                    text={value}\n                    id={p_key}\n                    key={`${p_key}-${i}-${index}`}\n                    value={String(index+1)}\n                />\n            })\n\n            let container_cls = \"p-q-container\"\n            if(lazyCheckList && lazyCheckList[p_key] === \"\"){\n                container_cls += \" fadeout_super\"\n            }\n\n            const problemDom = <div id={`Q${i}`} key={`${p_key}-${i}`} className={container_cls}>\n                    <div className=\"p-q-title-container\">\n                        <div className=\"p-header-container\">\n                            {!isEnd && <img src={problemClearList[i-1] ?  check_green_path : check_gray_path} className=\"check_top-icon\" alt=\"correct icon\" />}\n\n                            <h3 className=\"p-question-title\">問題 {i}</h3>\n                        </div>\n                        {isEnd &&\n                            <img src={checkList[`p${i}`] === answer[`p${i}`] ? check_green_light_path : check_wrong_path} className=\"check_icon_small\" alt=\"correct icon\"/>\n                        }\n                    </div>\n                    <p className=\"p-question-text\">\n                        <span dangerouslySetInnerHTML={{__html: q.question}} />\n                    </p>\n                    {q.img && <img className=\"p-img\" src={require(`../images/problems/${q.img}`)} />}\n                    {selectionDom}\n                </div>\n\n            problemDomList.push(problemDom)\n        }\n        return problemDomList\n    }\n// WRONG\n    return (\n        <div className=\"p-container\">\n            {/* =====  PROBLEM HEADER ===== */}\n            <div className=\"problem-title\">\n                <div className=\"problem-title-left\">\n                    {/* {isAllClear || isClear && <img src=\"check_green_path\" className=\"top-icon\" alt=\"correct icon\" \n                    />} */}\n                    <h2 className=\"p-title\">{title}</h2>\n                </div>\n                <Link to={'/'}>\n                    <img src={back_on_path} className=\"top-btn\"/>\n            </Link>\n            </div>\n           \n            {/* =========================== */}\n\n            <form onSubmit={onSubmit}>\n                <SeleclLabels />\n                <div className=\"btn-container\">\n                    {!isEnd && <button className=\"btn\" id=\"submit-btn\">答え合わせ</button>}\n                </div>\n\n                <div className=\"result-container\">\n                    {!isEnd && <span id=\"rewardId\" />}\n                    {isEnd && <h2 id=\"rewardId\" className=\"result-txt\">\n                        <span className=\"small-result-txt\">点数</span>\n                        <span className=\"result-num\">{correctCnt}</span>\n                        <span className=\"small-result-txt\">{`/ ${p_length}`}</span>\n                        {isClear && <span className=\"reward-btn\" onClick={reward}> 🎉</span>}\n                    </h2>}\n                    \n                    {(isEnd && isClear && isLastChapter) && \n                        <Link to={`/`}>\n                        <button className=\"btn result-btn\">目次へ</button>\n                        </Link>\n                    }\n                    {(isEnd && isClear && !isLastChapter) && \n                        <Link to={`/chapter${chapter + 1}/`}>\n                        <button className=\"btn result-btn\">次の問題へ</button>\n                        </Link>\n                    }\n\n                    {(isEnd && !isClear) && <button onClick={reset} className=\"btn result-btn\">やり直す</button>}\n                    \n                </div>\n            </form>\n            <Footer />\n        </div>\n    )\n}","import React from \"react\"\nimport '../css/ErrorPage.css';\nimport { Link } from \"react-router-dom\";\n\n\nexport default function ErrorPage() {\n    return (\n        <div className=\"error-container\">\n            <h1>404</h1>\n            <p>page not found</p>\n\n            <Link to={'/'}>\n            <button className=\"btn result-btn\">HOME</button>\n            </Link>\n        </div>\n   )\n}","import React from 'react';\nimport TopPage from './TopPage';\nimport ProblemPage from './ProblemPage'\nimport ErrorPage from './ErrorPage';\nimport { BrowserRouter, Routes, Route } from \"react-router-dom\";\n\nimport '../css/App.css';\n\n\nexport default　function App() {\n    const titleList ={\n      data: [\n      \"1章 バンディット問題\",\n      \"2章 マルコフ決定過程\",\n      \"3章 ベルマン方程式\",\n      \"4章 動的計画法\",\n      \"5章 モンテカルロ法\",\n      \"6章 TD法\",\n      \"7章 ニューラルネットとQ学習\",\n      \"8章 DQN\",\n      \"9章 方策勾配法\",\n      \"10章 さらに先へ\",\n  ]\n}\n\n  const pages = []\n  for(let i=0; i <titleList.data.length; i++){\n    const p = <Route key={i} path={`/chapter${i+1}/`} element={<ProblemPage chapter={i+1} title={titleList.data[i]}/>} />\n    pages.push(p)\n  }\n\n  return (\n    <BrowserRouter basename={process.env.PUBLIC_URL}>\n      <Routes>\n        <Route path={`/`} element={<TopPage titleList={titleList} />} />\n        {pages}\n        <Route path='*' element={<ErrorPage />} />\n      </Routes>\n    </BrowserRouter>\n\n\n    // <div className=\"App\"> \n    //   <TopPage clickHandler={chapterSelect}/>\n    // </div>\n  );\n}","import React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport './css/index.css';\nimport App from './components/App';\n\n\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n  // <React.StrictMode>\n    <App />\n  // </React.StrictMode>\n);\n\n"],"sourceRoot":""}